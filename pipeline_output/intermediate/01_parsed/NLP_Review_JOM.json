[
  {
    "text": "See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/355098954",
    "fontname": "SourceSansPro-Regular",
    "fontsize": 5.96,
    "page_number": 0,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 5.96,
      "indentation": 39.76,
      "is_bold": 0,
      "y_coordinate": 767.11,
      "centering_offset": 110.03,
      "word_count": 11
    }
  },
  {
    "text": "Challenges and Advances in Information Extraction from Scientiﬁc Literature:\na Review",
    "fontname": "Martel-Regular",
    "fontsize": 13.25,
    "page_number": 0,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 13.25,
      "indentation": 39.76,
      "is_bold": 0,
      "y_coordinate": 742.08,
      "centering_offset": 0.39,
      "word_count": 11
    }
  },
  {
    "text": "Article  in  JOM: the journal of the Minerals, Metals & Materials Society · October 2021",
    "fontname": "SourceSansPro-Regular",
    "fontsize": 6.63,
    "page_number": 0,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 6.63,
      "indentation": 39.76,
      "is_bold": 0,
      "y_coordinate": 694.05,
      "centering_offset": 141.46,
      "word_count": 15
    }
  },
  {
    "text": "DOI: 10.1007/s11837-021-04902-9",
    "fontname": "SourceSansPro-Regular",
    "fontsize": 4.64,
    "page_number": 0,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 4.64,
      "indentation": 39.76,
      "is_bold": 0,
      "y_coordinate": 682.81,
      "centering_offset": 225.56,
      "word_count": 2
    }
  },
  {
    "text": "CITATIONS",
    "fontname": "SourceSansPro-Regular",
    "fontsize": 5.3,
    "page_number": 0,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 5.3,
      "indentation": 39.76,
      "is_bold": 0,
      "y_coordinate": 650.75,
      "centering_offset": 245.86,
      "word_count": 1
    }
  },
  {
    "text": "48",
    "fontname": "SourceSansPro-Regular",
    "fontsize": 7.95,
    "page_number": 0,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.95,
      "indentation": 39.76,
      "is_bold": 0,
      "y_coordinate": 641.78,
      "centering_offset": 253.78,
      "word_count": 1
    }
  },
  {
    "text": "5 authors, including:\nLogan Ward\nArgonne National Laboratory",
    "fontname": "SourceSansPro-Regular",
    "fontsize": 6.63,
    "page_number": 0,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 6.63,
      "indentation": 39.76,
      "is_bold": 0,
      "y_coordinate": 615.86,
      "centering_offset": 204.28,
      "word_count": 8
    }
  },
  {
    "text": "147 PUBLICATIONS   7,031 CITATIONS\nREADS",
    "fontname": "SourceSansPro-Regular",
    "fontsize": 5.3,
    "page_number": 0,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 5.3,
      "indentation": 66.26,
      "is_bold": 0,
      "y_coordinate": 650.75,
      "centering_offset": 108.18,
      "word_count": 5
    }
  },
  {
    "text": "1,843",
    "fontname": "SourceSansPro-Regular",
    "fontsize": 7.95,
    "page_number": 0,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.95,
      "indentation": 297.5,
      "is_bold": 0,
      "y_coordinate": 641.78,
      "centering_offset": 8.9,
      "word_count": 1
    }
  },
  {
    "text": "Kyle Chard\nUniversity of Chicago",
    "fontname": "SourceSansPro-Regular",
    "fontsize": 6.63,
    "page_number": 0,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 6.63,
      "indentation": 324.0,
      "is_bold": 0,
      "y_coordinate": 597.9,
      "centering_offset": 55.64,
      "word_count": 5
    }
  },
  {
    "text": "310 PUBLICATIONS   6,455 CITATIONS\nSEE PROFILE\nSEE PROFILE",
    "fontname": "SourceSansPro-Regular",
    "fontsize": 5.3,
    "page_number": 0,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 5.3,
      "indentation": 73.55,
      "is_bold": 0,
      "y_coordinate": 575.37,
      "centering_offset": 53.36,
      "word_count": 8
    }
  },
  {
    "text": "Ben Blaiszik\nUniversity of Chicago",
    "fontname": "SourceSansPro-Regular",
    "fontsize": 6.63,
    "page_number": 0,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 6.63,
      "indentation": 66.26,
      "is_bold": 0,
      "y_coordinate": 534.95,
      "centering_offset": 202.11,
      "word_count": 5
    }
  },
  {
    "text": "116 PUBLICATIONS   6,363 CITATIONS",
    "fontname": "SourceSansPro-Regular",
    "fontsize": 5.3,
    "page_number": 0,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 5.3,
      "indentation": 66.26,
      "is_bold": 0,
      "y_coordinate": 512.43,
      "centering_offset": 185.88,
      "word_count": 4
    }
  },
  {
    "text": "Ian Foster\nUniversity of Chicago",
    "fontname": "SourceSansPro-Regular",
    "fontsize": 6.63,
    "page_number": 0,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 6.63,
      "indentation": 324.0,
      "is_bold": 0,
      "y_coordinate": 534.95,
      "centering_offset": 55.64,
      "word_count": 5
    }
  },
  {
    "text": "1,243 PUBLICATIONS   110,053 CITATIONS\nSEE PROFILE\nSEE PROFILE",
    "fontname": "SourceSansPro-Regular",
    "fontsize": 5.3,
    "page_number": 0,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 5.3,
      "indentation": 73.55,
      "is_bold": 0,
      "y_coordinate": 512.43,
      "centering_offset": 47.95,
      "word_count": 8
    }
  },
  {
    "text": "All content following this page was uploaded by Logan Ward on 12 January 2022.",
    "fontname": "SourceSansPro-Regular",
    "fontsize": 6.63,
    "page_number": 0,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 6.63,
      "indentation": 43.07,
      "is_bold": 0,
      "y_coordinate": 36.03,
      "centering_offset": 143.4,
      "word_count": 14
    }
  },
  {
    "text": "The user has requested enhancement of the downloaded file.",
    "fontname": "SourceSansPro-Regular",
    "fontsize": 5.3,
    "page_number": 0,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 5.3,
      "indentation": 43.07,
      "is_bold": 0,
      "y_coordinate": 19.97,
      "centering_offset": 186.89,
      "word_count": 9
    }
  },
  {
    "text": "JOM manuscript No.",
    "fontname": "AGPWOJ+CMBX9",
    "fontsize": 9.46,
    "page_number": 1,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 75.39,
      "is_bold": 0,
      "y_coordinate": 788.96,
      "centering_offset": 170.08,
      "word_count": 3
    }
  },
  {
    "text": "(will be inserted by the editor)",
    "fontname": "VMRAWK+CMR9",
    "fontsize": 9.46,
    "page_number": 1,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 75.39,
      "is_bold": 0,
      "y_coordinate": 778.3,
      "centering_offset": 156.67,
      "word_count": 6
    }
  },
  {
    "text": "Challenges and Advances in Information Extraction\nFrom Scientiﬁc Literature: A Review",
    "fontname": "TVWQHD+CMBX12",
    "fontsize": 12.95,
    "page_number": 1,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 12.95,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 718.21,
      "centering_offset": 59.43,
      "word_count": 11
    }
  },
  {
    "text": "Zhi Hong · Logan Ward · Kyle Chard ·\nBen Blaiszik · Ian Foster",
    "fontname": "AGPWOJ+CMBX9",
    "fontsize": 9.46,
    "page_number": 1,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 671.44,
      "centering_offset": 125.01,
      "word_count": 14
    }
  },
  {
    "text": "Received: date / Accepted: date",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 1,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 595.5,
      "centering_offset": 166.19,
      "word_count": 5
    }
  },
  {
    "text": "Abstract Scientiﬁc articles have long been the primary means of disseminating\nscientiﬁc discoveries. Over the centuries, valuable data and potentially ground-\nbreaking insights have been collected and buried deep in the mountain of publica-\ntions. In materials engineering, such data are spread across technical handbooks\nspeciﬁcation sheets, journal articles, and laboratory notebooks in myriad for for-\nmats. Extracting information from papers on a large scale has been a tedious\nand time-consuming job to which few researchers wanted to devote their limited\ntime and eﬀort, yet is an activity that is essential for modern data-driven design\npractices. However, in recent years, signiﬁcant progress has been made by the\ncomputer science community on techniques for automated information extraction\nfrom free text. Yet, transformative application of these techniques to scientiﬁc lit-\nerature remains elusive—due not to a lack of interest or eﬀort, but to technical\nand logistical challenges. Using the challenges in the materials science literature as\na driving motivation, we review the gaps between state-of-the-art information ex-\ntraction methods and the practical application of such methods to scientiﬁc texts,\nand oﬀer a comprehensive overview of work that can be undertaken to close these\ngaps.\nKeywords information extraction · text mining · scientiﬁc data",
    "fontname": "VMRAWK+CMR9",
    "fontsize": 9.46,
    "page_number": 1,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 564.56,
      "centering_offset": 52.78,
      "word_count": 201
    }
  },
  {
    "text": "1 Introduction",
    "fontname": "AGPWOJ+CMBX9",
    "fontsize": 9.46,
    "page_number": 1,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 322.53,
      "centering_offset": 190.32,
      "word_count": 2
    }
  },
  {
    "text": "“There is an information overload in scientiﬁc literature” [1], according to Nature.\nA bibliometrics study shows that approximately 2.5 million new papers are pub-\nlished each year [2]. Such enormous volumes of new information are well beyond\nany human’s ability to read, let alone to digest and absorb. Inevitably, valuable",
    "fontname": "VMRAWK+CMR9",
    "fontsize": 9.46,
    "page_number": 1,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 299.27,
      "centering_offset": 52.78,
      "word_count": 50
    }
  },
  {
    "text": "Z. Hong* · Kyle Chard · Ben Blaiszik · Ian Foster\nUniversity of Chicago, Chicago, IL, USA\nE-mail: *hongzhi@uchicago.edu\nLogan Ward · Kyle Chard · Ben Blaiszik · Ian Foster\nArgonne National Laboratory, Lemont, IL, USA",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 1,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 242.44,
      "centering_offset": 127.39,
      "word_count": 36
    }
  },
  {
    "text": "2\nZhi Hong et al.",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 2,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 774.32,
      "centering_offset": 52.78,
      "word_count": 5
    }
  },
  {
    "text": "knowledge remains buried deep in the publication mountain. As research tech-\nniques that require large quantities of scientiﬁc data gain popularity, automating\nthe process of retrieving pertinent information from free (i.e., natural language\nand unstructured) texts to feed said techniques is expected to become crucial to\nmany research domains.\nThe need for resources of well organized and thoroughly-vetted data are partic-\nularly evident in materials engineering. On one level, the identiﬁcation of appropri-\nate materials for new technologies is accomplished by searching through reams of\ncertiﬁcation and testing data — a process made better only by making more data\navailable to engineers. The design of new materials is also intrinsically limited by\navailable data. Data are foundational to devising and validating the core tools by\nwhich materials are understood and engineered: structure/property relationships.\nMany computational modeling tools, such as CALPHAD and phase-ﬁeld models,\nare parameterized using large amounts of experimental data [3]. In recent years,\nthe Materials Genome Initiative has further increased the prominence of data-\ndriven materials research [4]. Overall, high-quality resource of materials data have\nbeen critical to the development of materials engineering and promise to become\neven more important in the future.\nDespite the central need for high-quality data, building new databases remains\na resource-intensive task. Curated data repositories are only available as collections\npublished after signiﬁcant eﬀort (e.g., Polymer Handbook [5], the ASM Hand-\nbooks), community-driven resources from speciﬁc research communities (e.g., the\nCrystallographic Open Database [6], CALPHAD databases [3]), and web-accessible\ndatabases created by individual research groups (e.g., the Open Quantum Materi-\nals Database [7], Polymer Genome [8], Materials Project [9]). Such databases are\nthe ideal case for research data: vetted data presented in well-documented formats\nwith predictable structure. However, the majority of important material prop-\nerty data remain strewn throughout decades of journal articles in ﬁelds spanning\nfrom fundamental materials physics to myriad specialized industrial applications.\nConsequently, maintaining the status quo with respect to materials data curation\nwould result in the depths of historical data remaining uncatalogued and much\nnew data slipping into obscurity after publication. New approaches are needed.\nAutomated information extraction (IE) techniques oﬀer a route for accelerat-\ning the curation of data contained in scientiﬁc literatures. IE, a process for the\nautomated extraction of structured information, including entities and relations\nbetween entities, from text, is a highly-developed topic in the natural language pro-\ncessing (NLP) community. Since the 1970s, a wide variety of techniques have been\nproposed for tackling this problem [10]. Traditional methods often require consid-\nerable target domain knowledge and the development of sets of domain-speciﬁc\nrules deﬁned manually from experience. More recently, with rapid growth in both\ndata volumes and computing power, statistical models using machine learning\n(ML) or deep learning (DL) have taken center stage.\nAdoption of automated IE methods in science and engineering varies greatly\nacross disciplines. For example, use in the life sciences is advanced due to work\non online biomedical bibliographic systems, biomedical knowledge representation,\nand text mining dating back to the 1960s [11–14], while work in some other ﬁelds\nhas barely started. Automated IE in materials science and engineering is only in\nits initial stages, with promising work in the ceramics and polymers community\n[15–17] and signiﬁcant opportunities in other types of materials.",
    "fontname": "VMRAWK+CMR9",
    "fontsize": 9.46,
    "page_number": 2,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 750.11,
      "centering_offset": 52.78,
      "word_count": 540
    }
  },
  {
    "text": "Title Suppressed Due to Excessive Length\n3\nFig. 1 The three steps in a Scientiﬁc Information Extraction (SciIE) pipeline: Pre-processing\n(blue), data curation (orange), and learning (green). Components marked with a red alarm\nsymbol are particularly challenging.",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 3,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 774.32,
      "centering_offset": 52.79,
      "word_count": 37
    }
  },
  {
    "text": "In this paper, we present an overview of the scientiﬁc IE (SciIE) process with a\nparticular emphasis on the challenges and opportunities for materials science and\nengineering. Our goal is to examine the speciﬁc challenges and relevant advances in\napplying state-of-the-art methods developed by computer scientists to real-world\nmaterials SciIE problems. Finally, we identify important open research areas that\nshould be explored to advance the application of SciIE.",
    "fontname": "VMRAWK+CMR9",
    "fontsize": 9.46,
    "page_number": 3,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 448.87,
      "centering_offset": 52.78,
      "word_count": 68
    }
  },
  {
    "text": "2 Scientiﬁc Information Extraction Workﬂow",
    "fontname": "AGPWOJ+CMBX9",
    "fontsize": 9.46,
    "page_number": 3,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 358.94,
      "centering_offset": 115.34,
      "word_count": 5
    }
  },
  {
    "text": "Before diving into the speciﬁc barriers faced by SciIE, we provide an overview\nof the common steps involved in a SciIE pipeline (Fig. 1): data preprocessing,\ncuration and annotation, and learning.\nThe ﬁrst step, preprocessing, is to break down scientiﬁc articles into chunks\nof clean text for later steps. In addition to text in the body of an article, scien-\ntiﬁc documents may also have ﬁgures, tables, and publisher embellishments (e.g.,\nlogos, running titles, page numbers). The ﬁrst step in preprocessing is thus to\nparse the document and extract the body text. This is particularly complicated\nin science due to the complex formats of scientiﬁc articles, which we discuss in\nSection 3. While tables and ﬁgures may also contain valuable facts, extracting\ninformation from them relies on a completely diﬀerent set of technologies, such as\ncomputer vision, which are beyond the scope of this review. Interested readers may\nrefer to [19–22] for relevant research. After document parsing, texts are then split",
    "fontname": "VMRAWK+CMR9",
    "fontsize": 9.46,
    "page_number": 3,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 336.29,
      "centering_offset": 52.78,
      "word_count": 161
    }
  },
  {
    "text": "4\nZhi Hong et al.\nFig. 2 Example tasks in SciIE (visualized with Prodi.gy [18]): (a) Classifying sentences based\non whether they mention polymers, (b) recognizing named entities such as polymers, and (c)\nidentifying relations between named entities (e.g., polymers, properties, property values).",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 4,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 774.32,
      "centering_offset": 52.78,
      "word_count": 43
    }
  },
  {
    "text": "into smaller units. This step is called tokenization. Sentence tokenization splits\npassages into sentences, which is the expected input format for many later steps.\nWord tokenization further splits sentences into tokens to reduce the entropy of\nthe vocabulary (e.g., “is,” “does,” “isn’t,” “doesn’t” can be tokenized into three\ntokens: “is,” “does,” and “n’t.”)\nOnce text are extracted and cleaned, the next step is to create the tools that\nallow for so-called intelligent processing of the language. The entire process of\nextracting information from text is accomplished by a pipeline of complementary\ntools rather than a single program that produces data directly from tokenized text.\nThe steps in these pipelines often include (Fig. 2):\n1. Vocabulary Generation assigns a vector to each unique token in the corpus.\nThe vectors are referred to as word embeddings and are a key prerequisite to\nother NLP tasks. Meanings are often inferred by measuring similarity in the\ncontexts in which words appear, or the substructure of words. Embeddings can\neven be studied to predict materials properties [23].\n2. Text Classiﬁcation assigns a label or score to an entire block of text. For ex-\nample, they could determine whether a block of text is an abstract or whether\nit contains the desired data.\n3. Named Entity Recognition (NER) classiﬁes whether a word or phrase belongs\nto a speciﬁc category. Categories could be broad (e.g., noun) or speciﬁc (e.g.,\nplace name, polymer).\n4. Relationship Extraction produces pairs of named entities that are connected by\na certain relationship. A common example of IE in materials science and en-\ngineering is to associate words that are materials with those that are property\nnames. Complex relationships can then be built from multiple pair relation-\nships, such as the material “iron” has property “density” with value “7.9 g/cc.”\nIn principle, it should be possible to manually deﬁne rules to perform each of\nthe tasks just listed. For example, units can be identiﬁed by matching words that\noptionally begin with k, m, or M and end with a character from a known list (e.g.,",
    "fontname": "VMRAWK+CMR9",
    "fontsize": 9.46,
    "page_number": 4,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 534.7,
      "centering_offset": 52.78,
      "word_count": 342
    }
  },
  {
    "text": "Title Suppressed Due to Excessive Length\n5",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 5,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 774.32,
      "centering_offset": 52.79,
      "word_count": 7
    }
  },
  {
    "text": "m, s, Ω). The complexity of human language, however, is too high to allow for the\nenumeration of a complete set of rules except in trivial cases (e.g., units). Rather,\nthe modern approach is to use ML techniques to learn such rules automatically\nfrom many examples.\nThe second major step in performing IE is curating and annotating enough\ndata to train ML models for each SciIE subtask. Tokenized texts are ﬁrst selected\nand annotated to form a so-called gold standard training set for supervised models.\nLabels could be per example (sentence) or per token, depending on the task. A\nsentence could be given the label “True” or “False” if we simply want to classify\nwhether it mentions any polymer. If we want to ﬁnd out which polymers are\nmentioned in a sentence, then each word will need a separate label indicating\nwhether it is a polymer or not. The annotation process is usually costly, and\ntime-consuming, and is especially diﬃcult for scientiﬁc data due to the expertise\nrequired, the limited bandwidth of the people who do have the expertise (see\nSection 4), and the rarity of desired data across the scientiﬁc literature (see",
    "fontname": "VMRAWK+CMR9",
    "fontsize": 9.46,
    "page_number": 5,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 750.11,
      "centering_offset": 52.78,
      "word_count": 193
    }
  },
  {
    "text": "Section 5).",
    "fontname": "AGPWOJ+CMBX9",
    "fontsize": 9.46,
    "page_number": 5,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 577.91,
      "centering_offset": 199.92,
      "word_count": 2
    }
  },
  {
    "text": "The ﬁnal step in the pipeline is model learning. Machine learning models for\nNLP are classiﬁed into two categories: supervised and unsupervised models. Most\nmodels used in IE are supervised, meaning labels are required to accompany the\ntraining data. For example, common NER approaches use the embeddings and\nother features of a word (e.g., length, whether it contains digits) and those of\nits context (i.e., words ahead of or behind it) as input into a simple ML model\nlike a decision tree or support vector machine (SVM) to predict whether that\nword belongs to a category. State-of-the-art techniques use neural networks that\nautomatically account for the context of a word (e.g., recurrent, convolutional\nnetworks) and are ﬂexible enough to express the exquisitely complicated model\nforms required for expressing language [24, 25].\nModern NLP research has focused on reducing the amount of data required\nto train supervised learning models. Fine-tuning methods take pre-trained data\nfrom a previous NLP problem and (re)train part of the model on annotated data\nspeciﬁc to the task at hand. Google’s BERT language model, which is trained\non the BooksCorpus [26] (800M words) and English Wikipedia (2500M words),\nwhen ﬁne-tuned with just 2432 relevant paper abstracts, achieved a F-1 score of\n74.7% on extracting and classifying chemical-protein interactions (e.g., “Regula-\ntor”, “Agonist”, “Antagonist”, etc.) from the CHEMPROT corpus [27]. The F-1\nscore, or F-score, is the harmonic mean of a model’s precision (the fraction of\ncorrectly-predicted positive examples among the all the examples that the model\npredicted as positive) and recall (the fraction of correctly-predicted positive exam-\nples among all the positive examples in the dataset). It is often used to measure\na model’s prediction accuracy. Unsupervised models such as Open Information\nExtraction (OpenIE) systems have also been gaining traction because they do not\nrequire labeled data. Such systems have been demonstrated to outperform other\ntraditional IE methods in multiple studies [28–30]. However promising, the use of\nthese small-dataset learning techniques for scientiﬁc tasks is complicated by the\nesoteric language often used in science as well as the variation in languages used\nto describe the same concept (see Section 6).\nFinally, the diﬃculty in accurately extracting information combined with the\nhigh standards for success mandate that NLP tools must be used with care. The\nkey challenge in applying models is often the sparsity of desired information in",
    "fontname": "VMRAWK+CMR9",
    "fontsize": 9.46,
    "page_number": 5,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 566.11,
      "centering_offset": 52.78,
      "word_count": 390
    }
  },
  {
    "text": "6\nZhi Hong et al.",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 6,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 774.32,
      "centering_offset": 52.78,
      "word_count": 5
    }
  },
  {
    "text": "the literature, which can lead to many false positives when the pipeline is run\non too many irrelevant papers. It is diﬃcult to know ahead of time which papers\ncontain the target data and where to ﬁnd it in the paper. We detail the approaches\nfor pre-selecting texts and post-processing outputs to improve extracted data in",
    "fontname": "VMRAWK+CMR9",
    "fontsize": 9.46,
    "page_number": 6,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 750.11,
      "centering_offset": 52.78,
      "word_count": 56
    }
  },
  {
    "text": "Section 5.",
    "fontname": "AGPWOJ+CMBX9",
    "fontsize": 9.46,
    "page_number": 6,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 704.28,
      "centering_offset": 201.81,
      "word_count": 2
    }
  },
  {
    "text": "In the following sections, we carefully examine the major challenges faced by\nSciIE, including the scarcity of labeled data, the sparsity of interested information\nin publications, and the diﬃculties of applying ML or DL models trained on general\ncorpora to scientiﬁc texts. Relevant advances are discussed as potential solutions\nto, or methods for alleviating, these challenges.",
    "fontname": "VMRAWK+CMR9",
    "fontsize": 9.46,
    "page_number": 6,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 692.17,
      "centering_offset": 52.78,
      "word_count": 56
    }
  },
  {
    "text": "3 Challenge #1: The Computer-(un)friendly Formats of Scientiﬁc\nTexts",
    "fontname": "AGPWOJ+CMBX9",
    "fontsize": 9.46,
    "page_number": 6,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 606.6,
      "centering_offset": 64.77,
      "word_count": 9
    }
  },
  {
    "text": "The data needed to train or use NLP models are texts, ideally clean, well-formed\ntexts that can be easily ingested by computers (and humans). Many generic NLP\ndatasets exist, such as the CoNLL-2003 dataset for training Named Entity Recog-\nnition (NER) models [31] and the TACRED dataset for relation extraction mod-\nels [32]. One common feature of these NLP datasets is that they are distributed\nas plain text and have well-deﬁned homogeneous internal formats. For example,\nin the CoNLL dataset, each word is placed on a separate line with an empty line\nat the end of each sentence, and on each line, the word is followed by three tags:\na part-of-speech tag, a syntactic tag, and a named entity tag. Such structured\ndatasets can be fed into a model with only a few lines of code and without any\nextra pre-processing. Feeding scientiﬁc literature to models, unfortunately, is quite\na diﬀerent story.\nUseful documents in material engineering are stored in a few diﬀerent kinds\nof documents each with diﬀerent processing challenges. Speciﬁcation sheets and\nhandbooks often express data in tabular formats instead of natural language and\nare best treated with special-purpose software tailored to their speciﬁc formats\ngiven the predictable form in which data are expressed. Information from journal\narticles, conference proceedings and technical report are held in text in a variety\nof document formats. Older articles are often only available as scanned images\nwhereas more modern articles are expressed in a variety of digital formats. As we\ndetail here, this multitude of formats for engineering text presents a major barrier\nto extracting knowledge.\n3.1 Historical Relic: Portable Document Format\nMost papers are shared digitally in the Portable Document Format (PDF). In\norder to strictly maintain document typesetting, PDF stores a ﬁxed layout of the\ncontent, including texts, ﬁgures, and tables. However, it does not record structural\ninformation (Fig. 3). While humans can tell if a number is part of the body,\na page number, or a line number based on visual clues, it is diﬃcult to do so\nprogrammatically. In the worst case (common for papers published prior to 2000),\na PDF ﬁle may be a scanned image of the printed copy. In such cases, Optical",
    "fontname": "VMRAWK+CMR9",
    "fontsize": 9.46,
    "page_number": 6,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 571.84,
      "centering_offset": 52.78,
      "word_count": 367
    }
  },
  {
    "text": "Title Suppressed Due to Excessive Length\n7\nFig. 3 PDF (left) ﬁles are designed to capture page layout, while HTML (right) ﬁles are\ndesigned to save logical structure. It is thus much easier to identify which parts of a ﬁle are\nthe desired body texts with the help of HTML tags.",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 7,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 774.32,
      "centering_offset": 52.79,
      "word_count": 51
    }
  },
  {
    "text": "Character Recognition (OCR) must be performed to recognize the letters and\nnumbers in the paper, prior to proceeding with the rest of the pipeline.\nThe complex layouts of scientiﬁc papers makes it diﬃcult to extract the actual\nnarrative from PDF ﬁles. The widely-adopted double-column format can confuse\nautomated methods for identifying the ﬂow of text blocks. Pages are often dec-\norated with publisher names, running titles, page numbers, etc. General-purpose\ntoolkits such as PDF2Text [33] are not equipped to handle such complexities and\nthus they often produce outputs in which useful body text is mixed with typeset-\nting embellishments.\nSystems speciﬁcally designed to extract from scientiﬁc articles commonly em-\nploy a layout-aware (e.g., two-column vs. one-column) approach. Both manually-\ndeﬁned rules and statistical models may be used to estimate the structural informa-\ntion missing in PDFs. For example, LA-PDFText [34] detects words belonging to\nthe same block based on their font, height, and horizontal and vertical distance to\nthe nearest words. Then, a rule-based classiﬁer categorizes each block into sections\n(e.g., Abstract, Introduction, Methods, Discussion), and ﬁnally texts classiﬁed into\nthe same sections are stitched together to form the ﬁnal clean output. Although\nexperimental evaluations show that LA-PDFText reaches an F-1 score of 91% in\nidentifying and classifying texts in scientiﬁc articles, it is still not perfect and re-\nquires manually deﬁning rules according to the typesetting style, which varies from\njournal to journal or even from time to time for the same journal. Thus, applying\nLA-PDFText to a large collection of heterogeneous PDF papers is impractical for\nmany purposes.",
    "fontname": "VMRAWK+CMR9",
    "fontsize": 9.46,
    "page_number": 7,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 476.63,
      "centering_offset": 52.78,
      "word_count": 260
    }
  },
  {
    "text": "8\nZhi Hong et al.",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 8,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 774.32,
      "centering_offset": 52.78,
      "word_count": 5
    }
  },
  {
    "text": "3.2 Rising Star: Markup Language Formats\nWith the development of the Internet and web-based technologies, scientiﬁc papers\nare increasingly available in HyperText Markup Language (HTML) or eXtensible\nMarkup Language (XML) formats. HTML is by far the most popular format on\nthe web since it adapts to the browser screen regardless of its form factors. Most\nimportantly, HTML tags unambiguously identify the diﬀerent elements on a page\n(e.g., <img> for images, <table> for tables, and <p> for paragraphs) (Fig. 3).\nThese tags simplify the process of identifying sections in a paper. Body texts can\nbe extracted without any of the “guess work” involved with other formats.\nHTML-formatted papers are commonly used as the source in many studies.\nBigGrams is a semi-supervised IE system designed to work with HTML inputs [35].\nA series of research focusing on human-machine hybrid IE pipelines uses HTML\npapers from the journal Macromolecules for extracting glass-transition temper-\natures of polymers [36, 37] and creating a training dataset for ML models [38].\nAnother study uses HTML papers from Elsevier journals to develop a Name En-\ntity Recognition (NER) model to extract and analyze datasets used in sociology\nstudies [39].",
    "fontname": "VMRAWK+CMR9",
    "fontsize": 9.46,
    "page_number": 8,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 750.11,
      "centering_offset": 52.78,
      "word_count": 191
    }
  },
  {
    "text": "4 Challenge #2: The Need for (and Lack of ) Training Data",
    "fontname": "AGPWOJ+CMBX9",
    "fontsize": 9.46,
    "page_number": 8,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 520.22,
      "centering_offset": 80.07,
      "word_count": 12
    }
  },
  {
    "text": "Modern artiﬁcial intelligence (AI) methods derives their “intelligence” from the\ndata on which they are trained. No model architecture, regardless of its sophistica-\ntion, can do better than random guessing without training data. Moreover, having\nthe data itself is often not suﬃcient. Many models are created for predictive pur-\nposes, i.e., to respond to a query: “Given x, predict y.” Having the data (x) alone\nis of limited utility without the corresponding labels (y). Such models are called\nsupervised models and modern machine learning tools can require thousands of\nexamples of (x, y) pairs.\nThe dearth of training data in materials engineering can be simply attributed\nto IE in materials being in its beginnings, though there are ways to rapidly ac-\ncelerate developing the training sets. In this section, we ﬁrst examine methods for\ncuration of labeled training data, discussing the diﬃculties involved and present-\ning progresses towards solving the diﬃculties. Then, we discuss models that can\ntrain on data without labels (unsupervised models) and analyze their strengths\nand limitations. Fig. 4 shows a summary of the pros and cons of diﬀerent solutions\nto the data collection and annotation problem.\n4.1 Harvesting Existing Data\nIn many scientiﬁc ﬁelds, there are eﬀorts to compile useful databases from pub-\nlications. For example, the Polymer Property Predictor and Database includes\nproprieties such as the polymer interaction parameter (χ) and the glass transition\ntemperature (Tg) extracted semi-automatically from literature [40]. In sociology,\nthe Inter-university Consortium for Political and Social Research (ICPSR) main-\ntains a database of sociology datasets and related publications via manual cura-\ntion [49]. These databases usually store the extracted data in a structured format,",
    "fontname": "VMRAWK+CMR9",
    "fontsize": 9.46,
    "page_number": 8,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 497.43,
      "centering_offset": 52.78,
      "word_count": 273
    }
  },
  {
    "text": "Title Suppressed Due to Excessive Length\n9",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 9,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 774.32,
      "centering_offset": 52.79,
      "word_count": 7
    }
  },
  {
    "text": "Training Data Collection\nHarvesting\nExisting Data\nCrowd\nSourcing\nComputer-aided\nData Collection\nDistant-supervised and\nUnsupervised Methods\nPros:\nPros:\nPros:\nPros:",
    "fontname": "EBIDMB+CMBX8",
    "fontsize": 7.97,
    "page_number": 9,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 95.25,
      "is_bold": 0,
      "y_coordinate": 749.09,
      "centering_offset": 27.6,
      "word_count": 19
    }
  },
  {
    "text": "1) Minimum eﬀort\nrequired",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 9,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 104.52,
      "is_bold": 0,
      "y_coordinate": 624.7,
      "centering_offset": 158.89,
      "word_count": 4
    }
  },
  {
    "text": "Cons:",
    "fontname": "EBIDMB+CMBX8",
    "fontsize": 7.97,
    "page_number": 9,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 104.52,
      "is_bold": 0,
      "y_coordinate": 593.97,
      "centering_offset": 181.16,
      "word_count": 1
    }
  },
  {
    "text": "1) Such data may be\nunavailable in the\ndesired domain",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 9,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 104.52,
      "is_bold": 0,
      "y_coordinate": 576.18,
      "centering_offset": 154.54,
      "word_count": 10
    }
  },
  {
    "text": "Examples:",
    "fontname": "EBIDMB+CMBX8",
    "fontsize": 7.97,
    "page_number": 9,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 104.52,
      "is_bold": 0,
      "y_coordinate": 536.24,
      "centering_offset": 171.32,
      "word_count": 1
    }
  },
  {
    "text": "1) Polymer Handbook [5]\n2) PPPDB [40]\n1) A fast, scalable way\nto get large\namounts of data\n2) Relatively low cost\n1) Reduces or eliminates\nmanual workload\n2) Interactively and iteratively\nreceives human input",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 9,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 104.52,
      "is_bold": 0,
      "y_coordinate": 632.68,
      "centering_offset": 22.31,
      "word_count": 35
    }
  },
  {
    "text": "Cons:\nCons:",
    "fontname": "EBIDMB+CMBX8",
    "fontsize": 7.97,
    "page_number": 9,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 213.71,
      "is_bold": 0,
      "y_coordinate": 583.52,
      "centering_offset": 12.45,
      "word_count": 2
    }
  },
  {
    "text": "1) Requires careful\ndecomposition of\nmegatasks\ninto microtasks\n2) Quality of collected\ndata may vary",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 9,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 213.71,
      "is_bold": 0,
      "y_coordinate": 569.1,
      "centering_offset": 42.65,
      "word_count": 15
    }
  },
  {
    "text": "Examples:",
    "fontname": "EBIDMB+CMBX8",
    "fontsize": 7.97,
    "page_number": 9,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 213.71,
      "is_bold": 0,
      "y_coordinate": 501.51,
      "centering_offset": 62.13,
      "word_count": 1
    }
  },
  {
    "text": "1) Annotating disease\nmentions in abstracts [41]\n2) Digitizing satellite\nimages [42]\n1) Updating model after\neach loop takes time\n2) Eﬃciency and eﬀectiveness\ndepend on model quality",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 9,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 213.71,
      "is_bold": 0,
      "y_coordinate": 569.1,
      "centering_offset": 30.73,
      "word_count": 28
    }
  },
  {
    "text": "Examples:",
    "fontname": "EBIDMB+CMBX8",
    "fontsize": 7.97,
    "page_number": 9,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 332.74,
      "is_bold": 0,
      "y_coordinate": 519.94,
      "centering_offset": 56.91,
      "word_count": 1
    }
  },
  {
    "text": "1) Rule and dictionary-\nbased methods [43, 44]\n2) Model-in-the-pipeline [45, 46]\n3) Model-in-the-loop [38, 47, 48]\n1) No manual labeling\nrequired",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 9,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 332.74,
      "is_bold": 0,
      "y_coordinate": 624.7,
      "centering_offset": 142.9,
      "word_count": 22
    }
  },
  {
    "text": "Cons:",
    "fontname": "EBIDMB+CMBX8",
    "fontsize": 7.97,
    "page_number": 9,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 465.78,
      "is_bold": 0,
      "y_coordinate": 593.97,
      "centering_offset": 180.1,
      "word_count": 1
    }
  },
  {
    "text": "1) Output can be\nuninformative and/or\nincoherent.\n2) Standardization and\nnormalization required.",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 9,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 465.78,
      "is_bold": 0,
      "y_coordinate": 579.55,
      "centering_offset": 216.96,
      "word_count": 12
    }
  },
  {
    "text": "Examples:",
    "fontname": "EBIDMB+CMBX8",
    "fontsize": 7.97,
    "page_number": 9,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 465.78,
      "is_bold": 0,
      "y_coordinate": 521.18,
      "centering_offset": 189.94,
      "word_count": 1
    }
  },
  {
    "text": "1) Standford OpenIE [30]\n2) TextRunner [28]\nFig. 4 Comparison of data collection and annotation solutions: Harvesting existing data takes\nthe least eﬀort if such data is available; crowdsourcing is the most straightforward, but requires\ncareful task decomposition; computer-aided methods reduce manual workload but quality may\nvary; distant-supervised and unsupervised methods do not require manual labeling but may\nproduce ambiguous or incoherent labels.",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 9,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 503.38,
      "centering_offset": 18.47,
      "word_count": 63
    }
  },
  {
    "text": "but they often have an identiﬁer (e.g., DOI) pointing to the source of the each\nentry, with which discovering the source text is feasible programmatically. Com-\nbining the curated data in the databases and their source text gets us a good gold\nstandard dataset for training ML or DL models to automatically extract similar\ndata from literature.\nAnother potential source of existing data comes from outside of academia.\nScientiﬁc publications have long been ignored by NLP research in industry. Yet\nin recent years some companies have developed models and datasets for scien-\ntiﬁc literature. For example, AllenAI developed SciBert [50] and published the\ndata used to train the model, which includes gold standard annotations on papers\nfrom semanticscholar.org for many common NLP tasks, such as labeled entities\nin sentences to develop Name Entity Recognition (NER) models, syntactic tags\nfor words in texts to train Syntax Parsing models, and classiﬁcation labels with\naccompanying sentences for Text Classiﬁcation.",
    "fontname": "VMRAWK+CMR9",
    "fontsize": 9.46,
    "page_number": 9,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 358.05,
      "centering_offset": 52.78,
      "word_count": 156
    }
  },
  {
    "text": "10",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 10,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 774.32,
      "centering_offset": 221.4,
      "word_count": 1
    }
  },
  {
    "text": "4.2 Crowdsourcing",
    "fontname": "VMRAWK+CMR9",
    "fontsize": 9.46,
    "page_number": 10,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 750.11,
      "centering_offset": 186.24,
      "word_count": 2
    }
  },
  {
    "text": "Zhi Hong et al.",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 10,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 361.49,
      "is_bold": 0,
      "y_coordinate": 774.32,
      "centering_offset": 91.96,
      "word_count": 4
    }
  },
  {
    "text": "Traditionally, datasets have been curated via slow and expensive manual processes.\nFor example, from 1989 to 1992, a team at the University of Pennsylvania spent\nthree years annotating a corpus of over 4.5 million English words with part-of-\nspeech (POS) and sentence skeletal parsing information [51]. The resulting Penn\nTreebank dataset is still widely used to train NLP models for POS tagging and\nsentence parsing today, almost 30 years later, because few can aﬀord the high cost\nof building a corpus of such scale.\nWith the increasing penetration rate of Internet-connected devices among the\npopulation, crowdsourcing has become a more viable approach to a number of\nlabor-intensive tasks, corpus annotation included. Online services such as Amazon\nMechanical Turk (AMT) and CrowdFlower (CF) oﬀer platforms to post crowd-\nsourcing jobs and to engage the public to contribute for monetary rewards. There\nare also tools that help streamline the crowdsourcing process. The GATE crowd-\nsourcing plugin automates the mapping of documents to crowdsourcing units and\ngenerates user interfaces for common NLP crowdsourcing tasks [52]. Crowdsourc-\ning has been shown to be eﬀective at solving the problem of training data annota-\ntion. Granted, the quality of annotations produced by an untrained crowd could\nvary, so it is common to assign the same tasks to multiple workers and apply a\nmajority voting system to improve annotation quality. For example, 145 AMT\nparticipants annotated a corpus of 593 biological abstracts for disease mentions,\nachieving an overall F-1 score of 87.2% with a cost of $0.066 per abstract per\nworker [53]. Another AMT project adapted the reCAPTCHA idea [41] to crowd-\nsource the digitization of satellite images and demonstrated that an untrained\npopulation could achieve an accuracy within 10% of that of geoinformatics ex-\nperts [42]. MIT and Amazon conducted a research on 10 widely-used machine\nlearning datasets including text, image, and audio datasets, and found that the\nlabeling error rate ranges from 0.15% to 10.12% with an average error rate of\n3.4% [54].\nCrowdsourcing, nevertheless, is not without its drawbacks, and applying it to\nannotate a scientiﬁc corpus requires careful planning. Crowdsourcing is best-suited\nto tasks that are microtasks, i.e., tasks that are both relatively simple and mod-\nest in scale. In contrast, scientiﬁc research often aims to solve megatasks, such as\n“build a database of all material properties from all published literature.” Even\n“extracting all polymer proprieties mentioned in one paper” is a megatask for\ncrowdsourcing. One solution to this mismatch is by crowdsourcing only to peo-\nple with enough scientiﬁc backgrounds. One such eﬀort engaged undergraduate\nstudents to extract the Flory-Huggins (χ) parameter of polymer blends from the\njournal Macromolecules [55]. CHEMDNER is another chemistry corpus that con-\ntains 10 000 PubMed abstracts annotated by about 50 scientists from around 30\ninstitutions [56]. The Synthesis Project from MIT oﬀers annotations for 230 mate-\nrial science papers [57]. The well-known Polymer Handbook listed 96 contributors\nfrom universities and research institutions worldwide [5]. The National Institute\nof Standards and Technology (NIST) Thermodynamic Research Center (TRC)\nmaintains a large database of available thermophysical property data extracted\nfrom articles manually selected for relevant content [58].\nCrowdsourcing to people with relevant domain expertise seems like a sure way\nto guarantee the quality of the results, but it is not without its own shortcomings.",
    "fontname": "VMRAWK+CMR9",
    "fontsize": 9.46,
    "page_number": 10,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 727.05,
      "centering_offset": 52.78,
      "word_count": 545
    }
  },
  {
    "text": "Title Suppressed Due to Excessive Length\n11",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 11,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 774.32,
      "centering_offset": 52.79,
      "word_count": 7
    }
  },
  {
    "text": "In addition to being drastically more expensive, it also restricts the eligible “crowd”\nto a small group, and such people typically have limited bandwidth for such tasks.\nIn our experience, it took three materials scientists over two months to annotate\njust 150 paragraphs for glass transition temperatures (Tg) of polymers, since it\nwas diﬃcult to ﬁnd time to work on it with their busy schedules.\nIn order to take full advantage of crowdsourcing, megatasks need to be divided\ninto smaller and simpler microtasks. One way of doing so is to create a game with\na purpose (GWAP) [59]. Carefully designed GWAPs have been used to address\nmany complex scientiﬁc problems that would otherwise be incomprehensible to an\nuntrained person. In biology, the multiple gene sequence alignment problem has\nbeen presented as a color-matching game to crowdsourced workers [60]. GWAPs\nhave also been used in annotating complex language resources [61]. When design-\ning GWAPs for complex scientiﬁc text annotation, the key is to decompose the\nmegatasks. For example, instead of assigning a worker a full paper, assign them a\nparagraph or even a sentence; instead of asking them to label every material prop-\nerty, assign only the tasks pertaining to a single property, so that they have less\ninformation to remember. Partitioning megatasks into microtasks also enables the\nmapping of microtasks to diﬀerent levels in a GWAP based on diﬃculty, giving the\nworkers a sense of achievement as they increase their skill, which can contribute\nto keeping them engaged.\n4.3 Computer-aided Training Data Collection",
    "fontname": "VMRAWK+CMR9",
    "fontsize": 9.46,
    "page_number": 11,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 750.11,
      "centering_offset": 52.78,
      "word_count": 253
    }
  },
  {
    "text": "Rule-based and Dictionary-based Methods. Annotating a corpus does not have to",
    "fontname": "XRMLYS+CMTI9",
    "fontsize": 9.46,
    "page_number": 11,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 475.58,
      "centering_offset": 52.79,
      "word_count": 11
    }
  },
  {
    "text": "be done entirely by humans. For certain tasks, the help of machines can greatly\nreduce the manual eﬀort required to annotate a corpus. In many science domains,\nsystematic nomenclatures and unique identiﬁers are commonly used. In chemistry,\nthere is the International Union of Pure and Applied Chemistry (IUPAC) nomen-\nclature [62] and Chemical Abstracts Service (CAS) registry numbers [63], both are\nused to refer to chemicals in literature. In biology, standardized nomenclature has\nbeen deﬁned for human gene mutations [44]. Rule-based approaches (e.g., regular\nexpressions) are a great ﬁt to automatically annotate their mentions in texts [43,\n44]. Rules can also be constructed with formal grammars [43], which works best\nwhen there are commonly adopted languages in a domain for presenting certain\ntypes of information in the literature.\nRule-based methods are not suﬃcient if the information we want to annotate\nhas no obvious pattern. However, there are dictionaries available in many domains.\nFor example, DBpedia—a structured database built from Wikipedia that includes\ncategories such as Chemical Compound, Mineral, Gene, and Protein [64]—can\nbe leveraged to label automatically such entities in free text. Granted, rules and\ndictionaries may not be 100% accurate or comprehensive, so manual review is often\nnecessary, but reviewing is still much more eﬃcient than manual labeling.\nModel-in-the-pipeline Methods: Researchers have recently explored the use of ML/DL\nmodels to create datasets that are then used to train bigger and better models.\nTheir workﬂows begin with the manual annotation of a small subset of the corpus.\nThe annotated texts are used to train a model, which may be a simpliﬁed or a\nsmaller version of the full model to be trained on the fully annotated corpus. The",
    "fontname": "VMRAWK+CMR9",
    "fontsize": 9.46,
    "page_number": 11,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 464.13,
      "centering_offset": 47.38,
      "word_count": 278
    }
  },
  {
    "text": "12\nZhi Hong et al.",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 12,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 774.32,
      "centering_offset": 52.78,
      "word_count": 5
    }
  },
  {
    "text": "trained model is then applied to the rest of the unlabeled corpus. For each sample\nin the unlabeled corpus, the model’s prediction is used to compute a metric that\nis then used to decide what human annotators should do with that sample [45].\nFor a classiﬁcation model, for instance, the metric could be the classiﬁer output\nprobabilities. Samples with probabilities below a threshold should undergo a thor-\nough manual annotation process while those with higher probabilities could be\nassigned to a diﬀerent group of (perhaps less experienced) annotators for a quick\nreview [46].\nModel-in-the-loop Methods. Model-in-the-pipeline methods for corpus annotation\nhave a major drawback: the quality of the model in the pipeline is solely dependent\non the choice of the initial training examples. If the initially selected examples are\nnot representative of the overall distribution in the corpus, which is not unlikely for\nlarge corpora with hundreds of thousands of articles, the model will be less eﬀec-\ntive, and the human annotators would spend precious time on correcting the same\nmistake repeatedly. In contrast, model-in-the-loop methods avoid this problem by\nadding a feedback loop between humans and machine.\nWith the feedback loop, the initial selection of training examples becomes less\nsigniﬁcant. The annotation process is done in batches of m examples. The model\nruns prediction on a batch of examples, and after human review of the results,\nthe m gold standards are added to the training set and the model is re-trained\nwith the newly added data. This process is sometimes also called active learning,\nsince the model actively requests new inputs from humans in order to update\nitself [65]. Model-in-the-loop methods have been used to create training sets for\ndetecting mentions of polymers [38] and druglike molecules [47], extract health\ndeterminants [48], and detect named entities in ﬁnancial data [66].\n4.4 Distant-supervised and Unsupervised Methods\nWhile the aforementioned methods focus on reducing the cost of manual anno-\ntation through crowdsourcing, others seek to eliminate manual labor from the\nannotation process entirely. Distant-supervised methods still have supervised mod-\nels at the core, but the labels are automatically generated from an external source\nof knowledge such as Wikipedia or Freebase. If two entities in the same sentence\nmatch to an entry in the knowledge base, then the sentence is labeled as having\nthat relation [67].\nDistant-supervised methods nevertheless suﬀer from the noisy label problem.\nFor example, if say “(Bill Gates, Microsoft)” is described via a “FounderOf” re-\nlation in an external knowledge base, then the sentence “Bill Gates steps down\nfrom Microsoft board to focus on philanthropy” might be mislabeled as express-\ning that relation since it mentions both entities. Multi-instance learning (MIL) is\ndesigned to mitigate this problem. Instead of assigning a label to every training\nexample, examples that would get the same labels are put in a bag, and labels\nare assigned to the bag rather than the examples. The intuition is that some, but\nnot necessarily all, examples in the bag have that label [68]. Multi-instance multi-\nlabel learning takes this process one step further, assigning multiple possible labels\nto the same bag [69]. However, neither approach fully addresses the noisy labels",
    "fontname": "VMRAWK+CMR9",
    "fontsize": 9.46,
    "page_number": 12,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 750.11,
      "centering_offset": 52.78,
      "word_count": 524
    }
  },
  {
    "text": "Title Suppressed Due to Excessive Length\n13",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 13,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 774.32,
      "centering_offset": 52.79,
      "word_count": 7
    }
  },
  {
    "text": "problem. In both methods, labels are hard-coded to the bags and are immutable\nafter the distant-supervised labeling process.\nMore recently, distant-supervised learning with soft labels has been used to\ncorrect labels dynamically, instead of trying only to minimize the negative impact\nof mislabeled examples on model training [70]. Empirical data shows that distant-\nsupervised labeling assigns correct labels to a majority (94.4%) of the examples in\na benchmark [71]. With that assumption, the soft label of an example is updated\nbased on its syntax pattern similarity to the examples both in the same bag and\nin other bags.\nDistant-supervised learning has been applied to SciIE tasks. In one materials\nNLP task, the application of distant supervision to a corpus of 3400 publicly ac-\ncessible articles on ScienceDirect resulted in the automatic labeling of about 5000\nsentences as expressing process-structure or structure-property relations. The sen-\ntences were then used to train several models that aim to generate processing-\nstructure-property-performance (PSPP) design charts for desired properties from\ntext [72]. In biology, a dataset consisting of 1728 examples produced from PubMed\nabstracts by using Protein Data Bank (PDB) as the distant supervision knowl-\nedge base was used to train a model for mining protein-residue associations from\nliterature [73]. Another eﬀort produced 450 examples of intra-sentence gene-drug\nrelations from literature using the Gene Drug Knowledge Database (GDKD) as\nthe knowledge base [74].\nA notable deﬁciency of distant-supervised learning is that relations not present\nin the knowledge base cannot be recognized, no matter how prevalent they are in\nthe corpus. Open Information Extraction (OpenIE) systems such as TextRun-\nner [28] and the Wikipedia-based Open Extractor (WOE) [29] represent one ap-\nproach to overcoming this deﬁciency. These systems are not bound by a pre-deﬁned\nvocabulary because they extract entities and the relation term, all from the text\nbased on syntactic dependency. OpenIE is especially advantageous in annotat-\ning scientiﬁc articles describing cutting edge or rapidly evolving research, where\nstructured knowledge bases have not been compiled. In a study on a collection\nof 12 007 abstracts, Stanford OpenIE [30] extracted 3116 relations, 65% of which\nwere missed by other extraction tools [75]. This result shows that incorporating\nOpenIE systems into the automated annotation pipeline can greatly expand the\nscope of the produced training set.\nOpenIE can help with the noisy label problem that plagues distant supervision.\nBecause OpenIE only gets the relation term from text, we can be more conﬁdent\nthat the relation extracted is actually expressed in that text. In the Bill Gates\nexample above, it is impossible for OpenIE to output a “(Bill Gates, founder,\nMicrosoft)” relation since “founder” does not exist in that sentence. Therefore,\nOpenIE can be used to remove, and potentially correct, the noisy labels derived\nfrom knowledge bases in distant-supervised learning.\nOpenIE systems also have shortcomings. They may extract words too broad to\nbe meaningful as relations, such as “is”, “has”, and “got”, rather than the actual\nrelations “is the author of”, “has a population of”, and “got funding from”. Inco-\nherent words may be extracted as a relation, such as “was central torpedo” from\nthe sentence “The Mark 14 was central to the torpedo scandal of the ﬂeet.” [76].\nFurthermore, the extracted relations can be diﬃcult for downstream models to use,\nsuch as when the semantically equivalent phrases “melting point of” and “melting\npoint is” are captured as distinct relations. One solution to such problems is to",
    "fontname": "VMRAWK+CMR9",
    "fontsize": 9.46,
    "page_number": 13,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 750.11,
      "centering_offset": 52.78,
      "word_count": 567
    }
  },
  {
    "text": "14\nZhi Hong et al.",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 14,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 774.32,
      "centering_offset": 52.78,
      "word_count": 5
    }
  },
  {
    "text": "combine domain-independent OpenIE with domain-speciﬁc knowledge. Domain-\nspeciﬁc relation mapping rules, class recognizers, and SciIE models can be used to\ncluster and categorize relations detected by OpenIE systems [77–79].",
    "fontname": "VMRAWK+CMR9",
    "fontsize": 9.46,
    "page_number": 14,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 750.11,
      "centering_offset": 52.79,
      "word_count": 29
    }
  },
  {
    "text": "5 Challenge #3: The Sparsity of Information of Interest in Literature",
    "fontname": "AGPWOJ+CMBX9",
    "fontsize": 9.46,
    "page_number": 14,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 687.46,
      "centering_offset": 55.04,
      "word_count": 11
    }
  },
  {
    "text": "SciIE must address a challenging range of scales. At one extreme, according to a\nbibliographical study on global publications from 2000 to 2018 by the National\nScience Foundation (NSF) [80], the scientiﬁc literature published in that time\nspan encompasses some 35.5M articles, which ultimately we would like to analyze\nin their entirety. At the other extreme, the literature associated with individual\ndisciplines and subdisciplines, each characterized by distinct vocabularies and con-\nventions for communicating information, are much smaller. For example, the same\nstudy ﬁnds that the materials science literature encompasses 1.1M articles during\nthat period: 3.1% of the total. A particular subdiscipline, such as polymer science,\naccounts for just a portion of those 1.1M articles, of which a yet smaller subset\ncontain information relevant to any speciﬁc question.\nThus, even if we can identify the relevant publications accurately and eﬃ-\nciently, the sparsity of interesting information in text can be yet another obstacle\nto eﬃcacious IE from scientiﬁc literature [81, 82]. One study on extracting glass\ntransition temperature (Tg) shows that only 64 (0.67%) of 9518 sentences in 31\npapers from Macromolecules contain both a polymer and its Tg value [83]. The\nremaining 99.33% of sentences are just noise for this task. Such imbalance is rare\nin standard NLP datasets that most state-of-the-art models are designed for and\ntrained on, but will be common in extracting data from other materials engineering\nliterature For comparison, the widely used SemEval 2010 Task 8 relation extrac-\ntion dataset only has 17.63% and 16.71% of “Other” sentences (i.e., not belonging\nto known relations in the dataset) in the training and test set, respectively [84].\nExtracting information of such sparsity will be diﬃcult for ML/DL models. The\nhigh percentage of noise in texts will lead to more false positives and thus diluting\nthe extraction results. To avoid this problem, it is advisable to apply a ﬁltering\nstep during preprocessing to remove as much noise as possible before feeding the\ntexts to an IE model.\nIn this section, we will provide a review of the techniques developed to ﬁlter\nout noisy texts. We start with traditional intuitive heuristic-based methods and\nexpand to modern statistical model-based methods.\n5.1 Heuristic-based Filtering\nArticle Structure-based Filtering. Scientiﬁc papers usually follows a structured for-\nmat made up of sections and subsections. Sections such as “Introduction,” “Re-\nlated Work,” and “Experimental Results” are widely used in many domains. Some\npublishers even have standardized section headings that every manuscript must\nhave. With some background knowledge we can tell with conﬁdence that some sec-\ntions will not have the information we want to extract: the “References” section\nis probably not the best place to look if we want to extract the synthesis process",
    "fontname": "VMRAWK+CMR9",
    "fontsize": 9.46,
    "page_number": 14,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 664.15,
      "centering_offset": 52.78,
      "word_count": 449
    }
  },
  {
    "text": "Title Suppressed Due to Excessive Length\n15",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 15,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 774.32,
      "centering_offset": 52.79,
      "word_count": 7
    }
  },
  {
    "text": "Text Filtering\nHeuristic Methods\nStatistical Models\nArticle Structure Filtering\nClassiﬁcation Models\nPros:",
    "fontname": "EBIDMB+CMBX8",
    "fontsize": 7.97,
    "page_number": 15,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 95.25,
      "is_bold": 0,
      "y_coordinate": 749.09,
      "centering_offset": 43.18,
      "word_count": 12
    }
  },
  {
    "text": "1) Can quickly remove large chunks of irrelevant text\n2) Computationally inexpensive",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 15,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 104.52,
      "is_bold": 0,
      "y_coordinate": 632.68,
      "centering_offset": 94.52,
      "word_count": 12
    }
  },
  {
    "text": "Pros:",
    "fontname": "EBIDMB+CMBX8",
    "fontsize": 7.97,
    "page_number": 15,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 323.31,
      "is_bold": 0,
      "y_coordinate": 641.89,
      "centering_offset": 36.74,
      "word_count": 1
    }
  },
  {
    "text": "1) Better recall than rule-based methods",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 15,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 323.31,
      "is_bold": 0,
      "y_coordinate": 632.68,
      "centering_offset": 101.06,
      "word_count": 6
    }
  },
  {
    "text": "Cons:",
    "fontname": "EBIDMB+CMBX8",
    "fontsize": 7.97,
    "page_number": 15,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 104.52,
      "is_bold": 0,
      "y_coordinate": 605.03,
      "centering_offset": 181.16,
      "word_count": 1
    }
  },
  {
    "text": "1) Ineﬀective for articles not strictly following\nconventional structures",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 15,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 104.52,
      "is_bold": 0,
      "y_coordinate": 595.82,
      "centering_offset": 108.15,
      "word_count": 9
    }
  },
  {
    "text": "Examples:",
    "fontname": "EBIDMB+CMBX8",
    "fontsize": 7.97,
    "page_number": 15,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 104.52,
      "is_bold": 0,
      "y_coordinate": 568.17,
      "centering_offset": 171.32,
      "word_count": 1
    }
  },
  {
    "text": "1) Filtering HTML texts by tags\n2) A multi-pass approach for PDF ﬁles [85]",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 15,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 104.52,
      "is_bold": 0,
      "y_coordinate": 558.95,
      "centering_offset": 112.9,
      "word_count": 14
    }
  },
  {
    "text": "Sentence-level Filtering\nPros:",
    "fontname": "EBIDMB+CMBX8",
    "fontsize": 7.97,
    "page_number": 15,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 104.52,
      "is_bold": 0,
      "y_coordinate": 528.22,
      "centering_offset": 143.03,
      "word_count": 3
    }
  },
  {
    "text": "1) More ﬁne-grained than Article Structure ﬁltering.\n2) Computationally inexpensive",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 15,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 104.52,
      "is_bold": 0,
      "y_coordinate": 504.58,
      "centering_offset": 95.72,
      "word_count": 10
    }
  },
  {
    "text": "Cons:",
    "fontname": "EBIDMB+CMBX8",
    "fontsize": 7.97,
    "page_number": 15,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 104.52,
      "is_bold": 0,
      "y_coordinate": 476.93,
      "centering_offset": 181.16,
      "word_count": 1
    }
  },
  {
    "text": "1) Requires more manually-deﬁned rules than\nArticle Structure Filtering\n2) Recall is often lower than statistical methods.",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 15,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 104.52,
      "is_bold": 0,
      "y_coordinate": 467.72,
      "centering_offset": 103.11,
      "word_count": 17
    }
  },
  {
    "text": "Cons:",
    "fontname": "EBIDMB+CMBX8",
    "fontsize": 7.97,
    "page_number": 15,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 323.31,
      "is_bold": 0,
      "y_coordinate": 614.25,
      "centering_offset": 37.63,
      "word_count": 1
    }
  },
  {
    "text": "1) Requires additional labeled data",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 15,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 323.31,
      "is_bold": 0,
      "y_coordinate": 605.03,
      "centering_offset": 90.92,
      "word_count": 5
    }
  },
  {
    "text": "Examples:",
    "fontname": "EBIDMB+CMBX8",
    "fontsize": 7.97,
    "page_number": 15,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 323.31,
      "is_bold": 0,
      "y_coordinate": 586.6,
      "centering_offset": 47.48,
      "word_count": 1
    }
  },
  {
    "text": "1) SVN, KNN, naive Bayes [89, 90]",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 15,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 323.31,
      "is_bold": 0,
      "y_coordinate": 577.38,
      "centering_offset": 89.91,
      "word_count": 7
    }
  },
  {
    "text": "Subjectivity Analysis\nPros:",
    "fontname": "EBIDMB+CMBX8",
    "fontsize": 7.97,
    "page_number": 15,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 323.31,
      "is_bold": 0,
      "y_coordinate": 555.87,
      "centering_offset": 71.03,
      "word_count": 3
    }
  },
  {
    "text": "1) Solving the problem from a unique perspective\noften neglected by other methods",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 15,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 323.31,
      "is_bold": 0,
      "y_coordinate": 532.23,
      "centering_offset": 117.57,
      "word_count": 13
    }
  },
  {
    "text": "Cons:",
    "fontname": "EBIDMB+CMBX8",
    "fontsize": 7.97,
    "page_number": 15,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 323.31,
      "is_bold": 0,
      "y_coordinate": 504.59,
      "centering_offset": 37.63,
      "word_count": 1
    }
  },
  {
    "text": "1) Need to be used in conjunction with other\nmethods to achieve the best performance\n2) Requires additional labeled data",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 15,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 323.31,
      "is_bold": 0,
      "y_coordinate": 495.37,
      "centering_offset": 109.32,
      "word_count": 20
    }
  },
  {
    "text": "Examples:",
    "fontname": "EBIDMB+CMBX8",
    "fontsize": 7.97,
    "page_number": 15,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 323.31,
      "is_bold": 0,
      "y_coordinate": 458.51,
      "centering_offset": 47.48,
      "word_count": 1
    }
  },
  {
    "text": "1)Applying subjectivity analysis in IE [91–94]",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 15,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 323.31,
      "is_bold": 0,
      "y_coordinate": 449.29,
      "centering_offset": 110.48,
      "word_count": 6
    }
  },
  {
    "text": "Examples:",
    "fontname": "EBIDMB+CMBX8",
    "fontsize": 7.97,
    "page_number": 15,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 104.52,
      "is_bold": 0,
      "y_coordinate": 430.86,
      "centering_offset": 171.32,
      "word_count": 1
    }
  },
  {
    "text": "1) Rule- and pattern-base sentence ﬁltering [86–88]",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 15,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 104.52,
      "is_bold": 0,
      "y_coordinate": 421.64,
      "centering_offset": 98.33,
      "word_count": 7
    }
  },
  {
    "text": "Data Programming\nPros:",
    "fontname": "EBIDMB+CMBX8",
    "fontsize": 7.97,
    "page_number": 15,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 323.31,
      "is_bold": 0,
      "y_coordinate": 427.77,
      "centering_offset": 66.94,
      "word_count": 3
    }
  },
  {
    "text": "1) Does not require that rules be independent\nor highly accurate\n2) Simple to implement",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 15,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 323.31,
      "is_bold": 0,
      "y_coordinate": 404.14,
      "centering_offset": 110.71,
      "word_count": 15
    }
  },
  {
    "text": "Cons:",
    "fontname": "EBIDMB+CMBX8",
    "fontsize": 7.97,
    "page_number": 15,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 323.31,
      "is_bold": 0,
      "y_coordinate": 367.28,
      "centering_offset": 37.63,
      "word_count": 1
    }
  },
  {
    "text": "1) Requires additional labeled data",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 15,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 323.31,
      "is_bold": 0,
      "y_coordinate": 358.06,
      "centering_offset": 90.92,
      "word_count": 5
    }
  },
  {
    "text": "Examples:",
    "fontname": "EBIDMB+CMBX8",
    "fontsize": 7.97,
    "page_number": 15,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 323.31,
      "is_bold": 0,
      "y_coordinate": 339.63,
      "centering_offset": 47.48,
      "word_count": 1
    }
  },
  {
    "text": "1) ELSIE [83]\nFig. 5 Comparison of text ﬁltering techniques. Heuristic methods are simple to implement,\nand often achieve higher precision but lower recall. Statistical models are more complex, and\ncommonly have higher recall but lower precision.",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 15,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 330.41,
      "centering_offset": 52.79,
      "word_count": 37
    }
  },
  {
    "text": "of a novel polymer. Therefore, a ﬁlter can be applied to remove texts in irrelevant\nsections to reduce potential noise in subsequent IE tasks.\nPapers in markup languages (HTML or XML) can be easily dissected into sec-\ntions because they have rich metadata describing the document structure. PDF\nﬁles, however, cannot (see Section 3.1). A multi-pass sieve approach has been pro-\nposed to classify texts in PDF ﬁles into proper sections. It demonstrated better",
    "fontname": "VMRAWK+CMR9",
    "fontsize": 9.46,
    "page_number": 15,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 258.13,
      "centering_offset": 52.78,
      "word_count": 74
    }
  },
  {
    "text": "16\nZhi Hong et al.",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 16,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 774.32,
      "centering_offset": 52.78,
      "word_count": 5
    }
  },
  {
    "text": "performance (measured by F-scores) than many ML classiﬁers, including SVM,\nnaive Bayes, J48, and logistic regression [85]. Structure-based ﬁltering can be ap-\nplied to articles in PDF formats as well with it.\nSentence Level Filtering. Section ﬁltering alone is not always enough since noisy\ntexts exists in every section, even the potentially useful ones. Sentence ﬁltering\noﬀers more ﬁne-grained control. With manually-deﬁned rules or patterns it can\nachieve high precision, but often at the cost of recall due to the small number of\nrules deﬁned [86, 87]. Statistical methods can automatically learn a large number\nof rules and thus increase recall, but precision is lost as a result. Many eﬀorts have\nbeen made to improve the quality of the rule-base ﬁlters. Dropping rules whose\nkeywords triggers more false positives than true positives, limiting the maximum\nlength of sentences that can be matched, and tuning the threshold of what is\nconsidered as a match are simple yet eﬀective tricks that enabled a rule-based\nﬁlter to outperform a more sophisticated method based on minimum description\nlength from information theory [88].\n5.2 Filtering with Statistical Models\nUnlike heuristic-based ﬁltering, statistical models do not rely on explicit rules.\nThey learn what is (and is not) interested information from the context embed-\nded in word embeddings. Traditional classiﬁers are the most intuitive choice for\nthis task, and more sophisticated methods such as subjectivity analysis and data\nprogramming have also been used recently.\nClassiﬁcation Models. Text fragment ﬁltering is a type of classiﬁcation task, so\nintuitively ML classiﬁers have been applied to this task. Popular traditional clas-\nsiﬁers include decision tree, SVM, k-nearest neighbors, naive Bayes, importance\nvalue index (IVI), and C4.5 [89, 90]. Over the years amendments have been made\nto their algorithms to improve performance speciﬁcally on text classiﬁcation, and\neach method has its own advantages. The Naive Bayes classiﬁer has been shown to\nhave the best performance without any feature selection, whereas when features\n(words) were selected by its capacity to independently characterize a class, the IVI\nclassiﬁer came to the top [95]. SVM works well on two-class classiﬁcation prob-\nlems, and decision tree does not require independent features to be eﬀective [96].\nIn short, no single classiﬁer has signiﬁcant advantages over all others, and the\nchoice of which classiﬁer to use should be made based on the features used and\nthe task at hand.\nSubjectivity Analysis. Another common source of error for IE systems is subjec-\ntive language such as opinions, arguments, and speculations, which should be\nexcluded when aiming to extract scientiﬁc facts. Removing subjective sentences\nwith a Naive Bayes classiﬁer increased the precision of a IE system by 10% while\nlosing less than 2% of recall [92]. Another series of study demonstrated that IE and\nsubjectivity analysis are mutually beneﬁcial because a subjectivity classiﬁer can\nbe bootstrapped from clues learnt by IE techniques, and used to improve precision\non IE systems [91–94].",
    "fontname": "VMRAWK+CMR9",
    "fontsize": 9.46,
    "page_number": 16,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 750.11,
      "centering_offset": 52.78,
      "word_count": 482
    }
  },
  {
    "text": "Title Suppressed Due to Excessive Length\n17",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 17,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 774.32,
      "centering_offset": 52.79,
      "word_count": 7
    }
  },
  {
    "text": "Data Programming. This alternative ML-based approach to ﬁltering text frag-\nments for IE is used in Snorkel, a system for rapid collection of training data [97,\n98]. Snorkel is a weakly supervised method since it does not require any manual la-\nbeling. Instead, users write labeling functions (LFs). A LF assigns labels to inputs.\nIt can be based on arbitrary rules or heuristics. Diﬀerent LFs can have unknown\naccuracies and not be independent of each other. For each input, Snorkel aggre-\ngates the labels from all LFs, learning about the performance of the diﬀerent LFs\nin the process, and eventually outputs a high quality label for the input.\nThe ensemble labeling towards scientiﬁc information extraction (ELSIE) sys-\ntem builds on Snorkel. Its goal is not to extract relations from sentences, but to\ndetermine whether a particular sentence exists in a text fragment. Instead of hav-\ning each LF independently classify a sentence and then denoising their outputs, it\ngroups its LFs into buckets, with each bucket responsible for determining whether\na part of the target relation exists in the sentence. All buckets collectively aim to\ndetermine if the target relation is expressed in the text. ELSIE can successfully\nﬁlter sentences with 94% recall and 87% F-1 [83].",
    "fontname": "VMRAWK+CMR9",
    "fontsize": 9.46,
    "page_number": 17,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 750.11,
      "centering_offset": 52.78,
      "word_count": 207
    }
  },
  {
    "text": "6 Challenge #4: The Diﬃculties of Applying Models Trained on\nGeneral Corpora to Scientiﬁc Text",
    "fontname": "AGPWOJ+CMBX9",
    "fontsize": 9.46,
    "page_number": 17,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 542.61,
      "centering_offset": 68.07,
      "word_count": 15
    }
  },
  {
    "text": "It is standard practice in most applications of NLP to re-use ML models trained\non plentifully available text (e.g., websites, newspapers), with only retraining per-\nformed to adapt the models to new domains. For general NLP, there are many\ndatasets (e.g., Penn Treebank [51], CoNLL [99], OntoNotes [100]), pre-trained\nword vectors (e.g., GloVe [101], FastText [102]), and pre-trained models (e.g.,\nBERT [103], Turing-NLG [104]) available online. However, their training corpora\nare usually taken from general sources such as newspapers [51], Twitter posts [105],\nonline reviews [106], and Wikipedia pages [107] rather than scientiﬁc publications.\nIn addition to common obstacles like long-distance dependencies [108] and polyse-\nmant disambiguation [109], the unique terminologies and language styles used in\nscientiﬁc publications pose unique challenges to the IE problem and make model\nre-use problematic. Elsevier has conducted an evaluation of OpenIE systems on\ntwo datasets: one consisting of 200 sentences randomly selected from Wikipedia,\nand the second made up of 220 sentences from the scientiﬁc literature for the 10\nmost published disciplines. Extractions were checked manually by ﬁve humans to\nguarantee evaluation accuracy, and the results showed that the extractors perform\nbetter on encyclopedic sentences (54% precision) than on scientiﬁc sentences (34%\nprecision) [110].\nMany IE challenges observed in other ﬁelds of science will certainly also be\nchallenges in materials engineering. There is plenty of terminology that is not spe-\nciﬁc to materials engineering and yet is critical to understanding the content of\nmaterials text. In previous work, we have also encountered many challenges in that\nthe same materials can be referred to by diﬀerent names (e.g., trade names, spec-\niﬁcation number, composition), which complicates both learning language models\nand organizing extracted data. Long-distance dependencies are also common in\nmaterials article, such as when the processing path for a material is described in\na separate section from where its properties are discussed. Journal articles require\nsigniﬁcant training for humans to understand well, and it is similar for machines.",
    "fontname": "VMRAWK+CMR9",
    "fontsize": 9.46,
    "page_number": 17,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 508.33,
      "centering_offset": 52.78,
      "word_count": 324
    }
  },
  {
    "text": "18\nZhi Hong et al.",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 18,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 774.32,
      "centering_offset": 52.78,
      "word_count": 5
    }
  },
  {
    "text": "Applying Generic NLP Resources to SciIE\nStandard datasets\nPre-trained embeddings\nPre-trained language models\nDiﬃculty level: High\nDiﬃculty level: Medium\nDiﬃculty level: Medium",
    "fontname": "EBIDMB+CMBX8",
    "fontsize": 7.97,
    "page_number": 18,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 95.25,
      "is_bold": 0,
      "y_coordinate": 749.09,
      "centering_offset": 5.29,
      "word_count": 22
    }
  },
  {
    "text": "1) Datasets compiled from\nnews articles, web pages, etc.\ncannot be used to develop\nor test SciIE Models",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 18,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 104.52,
      "is_bold": 0,
      "y_coordinate": 641.89,
      "centering_offset": 133.42,
      "word_count": 18
    }
  },
  {
    "text": "Solutions",
    "fontname": "EBIDMB+CMBX8",
    "fontsize": 7.97,
    "page_number": 18,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 104.52,
      "is_bold": 0,
      "y_coordinate": 592.74,
      "centering_offset": 173.56,
      "word_count": 1
    }
  },
  {
    "text": "1) Compile SciIE datasets\nfrom scientiﬁc texts",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 18,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 104.52,
      "is_bold": 0,
      "y_coordinate": 570.34,
      "centering_offset": 144.91,
      "word_count": 7
    }
  },
  {
    "text": "Examples",
    "fontname": "EBIDMB+CMBX8",
    "fontsize": 7.97,
    "page_number": 18,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 104.52,
      "is_bold": 0,
      "y_coordinate": 539.61,
      "centering_offset": 172.67,
      "word_count": 1
    }
  },
  {
    "text": "1) CoNLL-2003 [31]\n1) Many words in scientiﬁc\narticles have the same\nmeaning as in generic texts\n2) Some terminologies may be\nunique to their domain",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 18,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 104.52,
      "is_bold": 0,
      "y_coordinate": 641.89,
      "centering_offset": 67.8,
      "word_count": 26
    }
  },
  {
    "text": "Solutions",
    "fontname": "EBIDMB+CMBX8",
    "fontsize": 7.97,
    "page_number": 18,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 242.85,
      "is_bold": 0,
      "y_coordinate": 574.3,
      "centering_offset": 35.24,
      "word_count": 1
    }
  },
  {
    "text": "1) Train word embeddings\non scientiﬁc texts to\nenrich the pre-trained\nembeddings.",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 18,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 242.85,
      "is_bold": 0,
      "y_coordinate": 565.72,
      "centering_offset": 6.07,
      "word_count": 12
    }
  },
  {
    "text": "Examples",
    "fontname": "EBIDMB+CMBX8",
    "fontsize": 7.97,
    "page_number": 18,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 242.85,
      "is_bold": 0,
      "y_coordinate": 516.56,
      "centering_offset": 34.34,
      "word_count": 1
    }
  },
  {
    "text": "1) Global Vectors\n(GloVe) [31]\n2) Synthesis Project\nWord Vectors [111, 112]\n1) Recent language models are\npre-trained on millions or billions\nof texts, but with no special focus\non scientiﬁc articles.",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 18,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 242.85,
      "is_bold": 0,
      "y_coordinate": 641.89,
      "centering_offset": 80.51,
      "word_count": 32
    }
  },
  {
    "text": "Solutions",
    "fontname": "EBIDMB+CMBX8",
    "fontsize": 7.97,
    "page_number": 18,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 376.16,
      "is_bold": 0,
      "y_coordinate": 592.73,
      "centering_offset": 98.07,
      "word_count": 1
    }
  },
  {
    "text": "1) Pre-trained language models\ncan be ﬁne-tuned on a\nscientiﬁc corpus. Fine tuning\nrequires signiﬁcantly less data\nand computational power\ncompared to the initial training.",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 18,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 376.16,
      "is_bold": 0,
      "y_coordinate": 578.31,
      "centering_offset": 143.92,
      "word_count": 25
    }
  },
  {
    "text": "Examples",
    "fontname": "EBIDMB+CMBX8",
    "fontsize": 7.97,
    "page_number": 18,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 376.16,
      "is_bold": 0,
      "y_coordinate": 510.72,
      "centering_offset": 98.97,
      "word_count": 1
    }
  },
  {
    "text": "1) BERT [103]\n2) SciBERT [50]\nFig. 6 Challenges with applying generic NLP methods to SciIE: Datasets compiled from\nnon-scientiﬁc sources cannot be applied to train SciIE models; word embeddings pre-trained\non generic texts need to be enriched with embeddings trained on scientiﬁc texts to include\nterminologies; pre-trained language models requires ﬁne-tuning on scientiﬁc texts to achieve\nbetter understanding of domain-speciﬁc language.",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 18,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 492.93,
      "centering_offset": 43.28,
      "word_count": 62
    }
  },
  {
    "text": "In this section, we will examine the many gaps that prevent state-of-the-art\nNLP models from reaching their full potential on scientiﬁc literature, as well as\ntechniques proposed to bridge the gaps.\n6.1 Standardized Datasets in NLP\nAs discussed in Section 4, assembling an NLP dataset often requires considerable\ntime and eﬀort, which makes many researchers turn to precompiled datasets. Using\nthese datasets saves NLP researchers valuable time and resources and provides\na level playing ﬁeld for comparing model performance, so it is no surprise to\nﬁnd that many models are solely developed and tested on standardized datasets.\nHowever, an often-overlooked problem is that these carefully curated datasets are\nnot representative of text found “in the wild”. Understandably, curators want their\ndatasets to be of high quality, i.e., comprised of rich, balanced, and clean training\nexamples. However, this curation process can lead to datasets and thus models\nthat present a distorted view of how language and texts are used.",
    "fontname": "VMRAWK+CMR9",
    "fontsize": 9.46,
    "page_number": 18,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 391.65,
      "centering_offset": 52.78,
      "word_count": 158
    }
  },
  {
    "text": "Title Suppressed Due to Excessive Length\n19",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 19,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 774.32,
      "centering_offset": 52.79,
      "word_count": 7
    }
  },
  {
    "text": "As an example, CoNLL-2003 is a widely used dataset compiled from Reuters\nnews stories to train Named Entity Recognition models [31]. There are over 31 000\nentities (location, organization, person, and misc) among its 22 000 sentences, av-\neraging to about 1.5 entities per sentence. In contrast, when developing a polymer\nNER model, we found that around 84% of 12 000 sentences randomly selected\nfrom publications in the journal Macromolecules did not mention any polymer\nnames [39] - resulting in a qualitatively diﬀerent predictive task. Besides, natural\nlanguage is rarely cut and dry. For example, in the polymer NER example, should\n“polymer a” and “polymer 1” be classiﬁed as polymers or not? While meaning-\nless in isolation, these terms can be helpful for downstream tasks such as relation\nextraction with reference resolution. Such grey areas are usually excluded from\nstandardized datasets to keep the dataset “clean.” Consequently, models devel-\noped on such datasets expect all input to be as clean and noise-free, often leading\nto performance degradation. Perhaps the deﬁnition of dataset quality should be\nreconsidered to include how well datasets reﬂect languages as they are naturally\nused.\n6.2 Transfer Learning\nUnique domain-speciﬁc languages are at the root of many incompatibilities be-\ntween state-of-the-art NLP models and scientiﬁc literature. As discussed in the\nprevious section, widely used datasets in the NLP community are usually com-\npiled from non-scientiﬁc corpora. Yet the meaning of a word can vary drastically\nin diﬀerent domains: for example, “PS” can be polystyrene, PostScript, or Pho-\ntoShop, depending on who you ask. Due to this variation, models trained on a\ngeneric corpora cannot be directly applied to extract information from scientiﬁc\nliterature.\nTransfer learning is an ML method that repurposes a model to a diﬀerent task\nby reusing (transferring) knowledge from the task on which it was trained. Transfer\nlearning has been applied to a variety of tasks, including image recognition [113],\nmachine translation [114], and text classiﬁcation [115]. It has also shown promis-\ning results in scientiﬁc research, such as medical imaging [116], material property\nprediction [117], and material defect detection [118]. Roughly speaking, transfer\nlearning strategies can be divided into two classes, inductive and transductive. In-\nductive learning reuses a model for a diﬀerent task in the same domain, whereas\nin transductive learning, the model is applied to a diﬀerent domain but the task\nis similar to the one it has been trained on. Transductive learning is particularly\nhelpful in SciIE, since there is a plethora of models designed for various tasks but\ntrained on generic texts. Common methods for transductive learning in NLP in-\nclude leveraging pre-trained word embeddings and ﬁne-tuning pre-trained models.\n6.3 Word Embedding\nWord embedding models map human-friendly words to computer-friendly vectors,\nin which the senses of words are embedded. With training, the word embedding\nvectors are ﬁt to capture the meaning(s) of a word from the contexts it is used in.",
    "fontname": "VMRAWK+CMR9",
    "fontsize": 9.46,
    "page_number": 19,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 750.11,
      "centering_offset": 52.78,
      "word_count": 482
    }
  },
  {
    "text": "20\nZhi Hong et al.",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 20,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 774.32,
      "centering_offset": 52.78,
      "word_count": 5
    }
  },
  {
    "text": "Due to the diﬃculty of directly judging whether a numeric vector accurately repre-\nsents a word’s semantic and syntactic information, most word embedding models\nare trained for speciﬁcally designed tasks that are closely related to how languages\nare used. When vectors can achieve satisfactory performance on the training task,\nthey are presumed to be accurate representations of the original words. For ex-\nample, Word2Vec, one of the most popular traditional word embedding models, is\ntrained for predicting a missing word based on its context [119, 120]. After train-\ning, word vectors are derived from the weights of the hidden layers in the model\nand may be used in transfer learning applications.\nSince word vectors are learned from context, the more often and accurately they\nare represented in the training corpus, the higher quality they will be. Although\nword embedding models can be trained using unsupervised methods, training on\na large corpus is often infeasible because of the associated computational require-\nments [121]. Therefore, word vectors are the primary form of transfer learning in\nNLP and a number of word embedding models pre-trained on large quantities of\ntexts are available. The Google News vectors are 300-dimensional Word2Vec vec-\ntors pre-trained on the Google News dataset of over 100 billion words [122]. The\nGlobal Vectors (GloVe) model of word representations has been used to produce\nword embeddings pre-trained on over 900 billion words from the Web [101]. Fast-\nText extends the Word2Vec model with character n-gram embeddings and oﬀers\npre-trained word vectors for 157 languages [123]. In material science speciﬁcally,\nthe Synthesis Project oﬀers pre-trained embeddings of the Word2Vec, FastText,\nand ELMo models [111, 112].\nTransfer learning with word embeddings is based on the assumption that the\nmeaning of a word is invariant across corpora, so that the vector for a word learned\nfrom one text or corporate can be used to represent the same word in a diﬀerent\ntext. Also, unlike human dictionaries, where one word can have multiple deﬁ-\nnitions, dictionaries generated by word embedding models only have one vector\n(value) for each word (key) in the vocabulary. However, multi-sense words are in-\nherent in natural languages, and discipline-speciﬁc jargon makes the problem yet\nmore complex. Sense2Vec, proposed to disambiguate word vectors [109], creates\nmultiple vectors corresponding to the diﬀerent senses of a word. To this end, it\nrequires the training corpus to have disambiguation labels. Although some dis-\nambiguation can be automatically inferred from part-of-speech labels or sentence\nparsing information, other cases still require manual annotation, sacriﬁcing the\nmost valuable trait of word embedding—unsupervised training.\nAnother, more widely adopted, solution to the multi-sense word problem in\ntransfer learning is simply to increase the size of the word vectors, hoping that\nthe extra dimensions will allow a single vector to embed multiple senses. This\nsolution is more widely used than the sense vector solution because it does not\naﬀect the unsupervised training process. Instead, responsibility for learning the\nproper weights for the dimensions of the word vector space is handed oﬀ to the\ndownstream models. The downside of this method is that it inﬂates downstream\nmodels, but with the rapidly increasing computational power oﬀered by hardware\naccelerators like GPUs and TPUs, this trade-oﬀ is getting easier to bear.",
    "fontname": "VMRAWK+CMR9",
    "fontsize": 9.46,
    "page_number": 20,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 750.11,
      "centering_offset": 52.78,
      "word_count": 538
    }
  },
  {
    "text": "Title Suppressed Due to Excessive Length\n21",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 21,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 774.32,
      "centering_offset": 52.79,
      "word_count": 7
    }
  },
  {
    "text": "6.4 Language Model Fine Tuning\nMany potentially valuable data are lost during transfer learning with word embed-\ndings. Speciﬁcally, only the vector outputs of the word embedding model is kept\nand reused, while the parameters in other hidden layers are discarded after train-\ning. Also, word vectors are only used to initialize the ﬁrst layer of the downstream\nmodel and kept frozen during training, thus knowledge gained from training with\none downstream task cannot be transferred to another with word vectors. There-\nfore, the ﬁne-tuning of pre-trained language models has been gaining more traction\nas an alternative transfer learning method in NLP.\nState-of-the-art language models such as BERT [103] and SciBERT [50] are\ntransformer models based on the encoder-decoder architecture [124]. Like word\nembedding models, they are pre-trained for generic language tasks (such as pre-\ndicting a missing word). However, unlike word embedding models, they keep all\nthe learnt weights after pre-training, with their model architecture allowing the\nsame model to be applied to various tasks (e.g., sentence classiﬁcation, named en-\ntity recognition) via ﬁne-tuning. Fine-tuning is the process of re-training a model\nwith new data speciﬁc to the task at hand. Since the model has been pre-trained,\nﬁne-tuning only takes a fraction of the data and time needed to train the model\nfrom scratch. Fine-tuning can be done on all the layers, or, for encoder-decoder\nmodels, only on the top few layers near the output while all other layers are kept\nfrozen. In this way, more valuable information is transferred from pre-training\nto speciﬁc downstream tasks. This method is especially helpful for using large\nmodels eﬀectively. BERT is trained from scratch on a corpus of over 3.3 billion\nwords on 16 TPUs in four days. Not every downstream task has a suﬃciently\nlarge corpus or enough computation power to train such big models from scratch.\nFine-tuning BERT, on the other hand, takes just a couple of hours on one GPU;\nthus, ﬁne-tuning provides an eﬃcient way for other tasks to reap the beneﬁts of\nlarge, complex models.",
    "fontname": "VMRAWK+CMR9",
    "fontsize": 9.46,
    "page_number": 21,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 750.11,
      "centering_offset": 52.78,
      "word_count": 338
    }
  },
  {
    "text": "7 Enabling Better IE in Materials Research",
    "fontname": "AGPWOJ+CMBX9",
    "fontsize": 9.46,
    "page_number": 21,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 394.13,
      "centering_offset": 118.82,
      "word_count": 7
    }
  },
  {
    "text": "As described above, signiﬁcant challenges must be addressed before the data pub-\nlished over decades in various scientiﬁc literatures can be made readily accessible.\nMany challenges require improvements in NLP techniques, which are not neces-\nsarily the domain of researchers working in disciplines such as material science\nand engineering. However, we note a few actions that the materials community\ncan perform to make the most of the current state-of-the-art and to ensure steady\ninnovation in the future.\n7.1 Access to Well-Formatted Versions of Articles\nAccess to machine-readable full-text articles (i.e., in HTML, XML, or JSON for-\nmats) is a prerequisite for robust SciIE, but obtaining such access, when possible at\nall, often involves lengthy negotiations with publishers. Such barriers limit growth\nof this ﬁeld. Moves towards open publication of articles in ways that enable unfet-\ntered access, whether by traditional publishers or via technical-society-sponsored",
    "fontname": "VMRAWK+CMR9",
    "fontsize": 9.46,
    "page_number": 21,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 371.33,
      "centering_offset": 52.78,
      "word_count": 144
    }
  },
  {
    "text": "22\nZhi Hong et al.",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 22,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 774.32,
      "centering_offset": 52.78,
      "word_count": 5
    }
  },
  {
    "text": "pre-print repositories (e.g., ChemRxiv), will have major beneﬁts for the NLP com-\nmunity and for science and engineering more broadly. Where full open access is\nnot possible, streamlined licensing agreements or the establishment of repositories\nwhere researchers can perform analyses would make a big diﬀerence.\nOpen software practices can also contribute to lowering the cost of entry for\nscientiﬁc NLP research. Many NLP eﬀorts begin by creating tools to streamline\naccess to publisher APIs or to handle the idiosyncratic formats in which arti-\ncles are provided. Sharing such tools in a community-wide repository—which has\nbeen done with atomic-scale simulation outputs [125]—would remove the need for\nindividual research groups repeatedly to recreate and maintain such tools.\n7.2 Open Science Practices in Materials NLP Research\nBuilding a IE pipeline to solve a speciﬁc problem involves much work that can\nbe repurposed for other tasks. Tools to process articles in the formats used by\ndiﬀerent journals, word embeddings created from text from a speciﬁc literature,\nand ML models trained on relevant data can all be useful to others for new tasks.\nOf course, some elements of the pipeline (e.g., typeset versions of journal articles)\nare protected IP, but most are not. We have created a checklist of components\nthat should be released for a paper to best ensure progress by the community:\n1. Preprocessing codes used to render journal articles or other text into a form\nready to be used in IE pipelines.\n2. Training sets employed to develop NER and association models are not just\nneeded for reproducibility, but are also critical to speeding development of NLP\ntools. Labeled datasets are often the most resource-intensive step (see Section\n4) in NLP, which make them particularly valuable to share.\n3. Word embeddings that underpin most NLP models are useful both to analyze\nfor new materials [23] and to bootstrap new NLP eﬀorts.\n4. Trained models, the ﬁnal product of NLP development, should be published\nto ensure access to data does not backslide as tool developers move on to new\nprojects.\nFortunately, the materials IE community has already established a culture\nof sharing such tools. The CHEMDNER datasets that were crucial in launching\nthe ﬁeld of NLP for chemical data extraction are open [56]. Swain and Cole also\nreleased their SciIE pipeline as an open-source tool, ChemDataExtractor [126],\nthat many teams have found useful [36], and that has been used to build a bat-\ntery chemicals and properties database of over 290 000 records [17]. The Synthesis\nProject has also produced useful training datasets and tools releases [111, 57, 112].\nKononova et al. published a dataset of nearly 20 000 inorganic materials synthesis\nrecipes extracted from text [16]. We hope that these early, excellent examples of\nopen science will set the stage for establishing a vibrant research community in\nNLP for materials science and engineering.\n7.3 Publishing Structured Data\nUltimately, human language is an imperfect way to communicate materials data.\nAutomated processing of large quantities of data works best when data are in",
    "fontname": "VMRAWK+CMR9",
    "fontsize": 9.46,
    "page_number": 22,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 750.11,
      "centering_offset": 52.78,
      "word_count": 499
    }
  },
  {
    "text": "Title Suppressed Due to Excessive Length\n23",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 23,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 774.32,
      "centering_offset": 52.79,
      "word_count": 7
    }
  },
  {
    "text": "identical formats and co-located with all metadata needed to understand them.\nHomogeneity and conciseness are not features of human languages. Accordingly,\nbetter access to materials data may ultimately be achieved best not by processing\ntext documents but by encouraging humans to record data in computer languages.\nWe already see adoption of such processes within certain subﬁelds and projects\nin materials science and engineering. For example, inorganic crystal structures\nmust be reported in the Crystallographic Information File (CIF) [127] format\nwhen published in journals of the International Union of Crystallography (IUCr).\nDensity functional theory (DFT) computations are starting to be shared in their\nnative formats via domain-speciﬁc databases that provide such data in searchable\nformats, such as NOMAD [128] and the Materials Data Facility [129, 130]. Fur-\nther, many individuals are beginning to publish their data in machine-accessible\nresources. Such activities focused on publishing data in computer-readable for-\nmats are important [131, 132], but are outside the scope of this review. We do note\nthat many subdisciplines within materials science, such as thermochemistry [133]\nand atomic-scale modeling [134], are establishing community standards needed to\nbringing order to a larger proportion of materials engineering data.\nIn short, scientists can reduce the need for SciIE by publishing information\nin computer-accessible forms. Ideally, all data would be published in a format\nthat includes descriptions of how they were collected, presented according to the\nstandards of a science community, as in the IUCr and NOMAD examples above.\nPublishing a large fraction of materials data in such structured formats will be a\nformidable challenge, but one that can be performed thoughtfully and in parallel\nwith advancements in NLP. Electronic laboratory notebooks, laboratory informa-\ntion management systems, and robotics are particularly promising technologies to\nmake digital data and metadata more prevalent in materials engineering [135].",
    "fontname": "VMRAWK+CMR9",
    "fontsize": 9.46,
    "page_number": 23,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 750.11,
      "centering_offset": 52.78,
      "word_count": 298
    }
  },
  {
    "text": "8 Conclusions and Outlook",
    "fontname": "AGPWOJ+CMBX9",
    "fontsize": 9.46,
    "page_number": 23,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 428.02,
      "centering_offset": 159.75,
      "word_count": 4
    }
  },
  {
    "text": "Data are taking center stage in materials engineering and an increasing number of\nscience ﬁelds, yet vast amounts of data remain and continue to be buried in written\npapers, inaccessible to humans and machines. In this paper, we have examined the\nmajor factors obstructing practical applications of computer-aided information\nextraction on scientiﬁc corpora, including the ﬁle formats of scientiﬁc publications,\nthe lack of domain-speciﬁc training data, the sparsity of interested information\nin papers, as well as the diﬃculties inherent in transferring a model trained on\ngeneric texts to scientiﬁc literature. We reviewed potential solutions or remedies for\nthe problems, and discussed their strengths and weaknesses. We intend that this\npaper provide a clear overview of the current landscape of scientiﬁc information\nextraction and shine light on the many obstacles that future research eﬀorts can\ntake on.\nInformation extraction represents only an initial step towards using NLP to\naid research in science and engineering by taking on tasks currently performed by\nhuman scientists. Outside science, modern NLP technologies are being used for\nincreasingly complex tasks, such as answering questions in prose (e.g., Google’s\nquestion-answering search engine [136]). A future NLP tool could use facts and\nqualitative relationships extracted from papers to enable autonomous reasoning\nengines capable of producing and testing hypotheses from the literature, or iden-",
    "fontname": "VMRAWK+CMR9",
    "fontsize": 9.46,
    "page_number": 23,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 405.2,
      "centering_offset": 52.78,
      "word_count": 215
    }
  },
  {
    "text": "24\nZhi Hong et al.",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 24,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 774.32,
      "centering_offset": 52.78,
      "word_count": 5
    }
  },
  {
    "text": "tifying anomalies worthy of further investigation. Solving the challenges currently\ninhibiting our ability to extract information from papers would unlock such a\npotential future for AI in materials science and engineering.",
    "fontname": "VMRAWK+CMR9",
    "fontsize": 9.46,
    "page_number": 24,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 750.11,
      "centering_offset": 52.79,
      "word_count": 31
    }
  },
  {
    "text": "Acknowledgements This work was performed under ﬁnancial assistance award 70NANB19H005\nfrom the U.S. Department of Commerce, National Institute of Standards and Technology, as\npart of the Center for Hierarchical Materials Design (CHiMaD), and was also supported in part\nby the U.S. Department of Energy, Oﬃce of Science, Advanced Scientiﬁc Computing Research,\nunder Contract DE-AC02-06CH11357, and by the Joint Center for Energy Storage Research\n(JCESR), an Energy Innovation Hub funded by the U.S. Department of Energy (DOE), Oﬃce\nof Science, Oﬃce of Basic Energy Sciences.",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 24,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 699.83,
      "centering_offset": 46.55,
      "word_count": 84
    }
  },
  {
    "text": "Conﬂict of Interest",
    "fontname": "XRMLYS+CMTI9",
    "fontsize": 9.46,
    "page_number": 24,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 621.53,
      "centering_offset": 185.22,
      "word_count": 3
    }
  },
  {
    "text": "On behalf of all authors, the corresponding author states that there are no conﬂicts\nof interest.",
    "fontname": "VMRAWK+CMR9",
    "fontsize": 9.46,
    "page_number": 24,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 598.88,
      "centering_offset": 52.79,
      "word_count": 16
    }
  },
  {
    "text": "References",
    "fontname": "AGPWOJ+CMBX9",
    "fontsize": 9.46,
    "page_number": 24,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 9.46,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 554.68,
      "centering_offset": 199.33,
      "word_count": 1
    }
  },
  {
    "text": "1. E. Landhuis, Nature 535(7612), 457 (2016)",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 24,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 80.47,
      "is_bold": 0,
      "y_coordinate": 533.35,
      "centering_offset": 131.06,
      "word_count": 7
    }
  },
  {
    "text": "2. M. Ware, M. Mabe, The STM report: An overview of scientiﬁc and scholarly journal pub-",
    "fontname": "IIWCPO+CMTI8",
    "fontsize": 7.97,
    "page_number": 24,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 80.47,
      "is_bold": 0,
      "y_coordinate": 524.41,
      "centering_offset": 48.55,
      "word_count": 16
    }
  },
  {
    "text": "lishing (International Association of Scientiﬁc, Technical and Medical Publishers, Oxford,\nUnited Kingdom, 2015)\n3. G. Olson, Scripta Materialia 70, 1 (2014)\n4. J.J. de Pablo, N.E. Jackson, M.A. Webb, L.Q. Chen, J.E. Moore, D. Morgan, R. Jacobs,\nT. Pollock, D.G. Schlom, E.S. Toberer, J. Analytis, I. Dabo, D.M. DeLongchamp, G.A.\nFiete, G.M. Grason, G. Hautier, Y. Mo, K. Rajan, E.J. Reed, E. Rodriguez, V. Stevanovic,\nJ. Suntivich, K. Thornton, J.C. Zhao, npj Computational Materials 5(1) (2019)\n5. J. Brandrup, E.H. Immergut, E.A. Grulke (eds.), Polymer Handbook, 4th Edition, 4th\nedn. (Wiley, 2004)\n6. S. Graˇzulis, D. Chateigner, R.T. Downs, A.F.T. Yokochi, M. Quir´os, L. Lutterotti,\nE. Manakova, J. Butkus, P. Moeck, A.L. Bail, Journal of Applied Crystallography 42(4),\n726 (2009)\n7. S. Kirklin, J.E. Saal, B. Meredig, A. Thompson, J.W. Doak, M. Aykol, S. R¨uhl, C. Wolver-\nton, npj Computational Materials 1(1), 1 (2015)\n8. C. Kim, A. Chandrasekaran, T.D. Huan, D. Das, R. Ramprasad, The Journal of Physical\nChemistry C 122(31), 17575 (2018)\n9. A. Jain, S.P. Ong, G. Hautier, W. Chen, W.D. Richards, S. Dacek, S. Cholia, D. Gunter,\nD. Skinner, G. Ceder, et al., APL Materials 1(1), 011002 (2013)\n10. C. Borkowski, J. Sperling Martin, Journal of the American Society for Information Science\n26(2), 94 (1975)\n11. F.B. Rogers, Bulletin of the Medical Library Association 52(1), 150 (1964)\n12. R.J. Roberts, Proceedings of the National Academy of Sciences 98(2), 381 (2001). DOI",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 24,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 76.23,
      "is_bold": 0,
      "y_coordinate": 515.19,
      "centering_offset": 50.67,
      "word_count": 235
    }
  },
  {
    "text": "10.1073/pnas.98.2.381. URL https://www.pnas.org/content/98/2/381",
    "fontname": "CMBMGW+CMTT8",
    "fontsize": 7.97,
    "page_number": 24,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 91.92,
      "is_bold": 0,
      "y_coordinate": 315.2,
      "centering_offset": 73.27,
      "word_count": 3
    }
  },
  {
    "text": "13. D.R. Swanson, N.R. Smalheiser, Artiﬁcial Intelligence 91(2), 183 (1997)\n14. L. Tanabe, U. Scherf, L. Smith, J. Lee, L. Hunter, J. Weinstein, Biotechniques 27(6),\n1210 (1999)\n15. E.A. Olivetti, J.M. Cole, E. Kim, O. Kononova, G. Ceder, T.Y.J. Han, A.M. Hiszpanski,\nApplied Physics Reviews 7(4), 041317 (2020)\n16. O. Kononova, H. Huo, T. He, Z. Rong, T. Botari, W. Sun, V. Tshitoyan, G. Ceder,\nScientiﬁc Data 6(1), 1 (2019)\n17. S. Huang, J.M. Cole, Scientiﬁc Data 7(1), 1 (2020)\n18. Prodi.gy. Prodi.gy: An annotation tool for AI, Machine Learning, and NLP. https:\n//prodi.gy (2021). Online; accessed 02-May-2021\n19. C.A. Clark, S.K. Divvala, in AAAI Workshop: Scholarly Big Data, vol. 6 (2015), vol. 6",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 24,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 76.23,
      "is_bold": 0,
      "y_coordinate": 306.26,
      "centering_offset": 50.67,
      "word_count": 114
    }
  },
  {
    "text": "20. Y. Liu, K. Bai, P. Mitra, C.L. Giles, in 7th ACM/IEEE-CS Joint Conference on Digital",
    "fontname": "IIWCPO+CMTI8",
    "fontsize": 7.97,
    "page_number": 24,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 76.23,
      "is_bold": 0,
      "y_coordinate": 206.82,
      "centering_offset": 50.67,
      "word_count": 16
    }
  },
  {
    "text": "libraries (2007), pp. 91–100",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 24,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 91.92,
      "is_bold": 0,
      "y_coordinate": 197.6,
      "centering_offset": 154.67,
      "word_count": 4
    }
  },
  {
    "text": "Title Suppressed Due to Excessive Length\n25\n21. B. Gatos, D. Danatsas, I. Pratikakis, S.J. Perantonis, in International Conference on",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 25,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 774.32,
      "centering_offset": 52.79,
      "word_count": 20
    }
  },
  {
    "text": "Pattern Recognition and Image Analysis (Springer, 2005), pp. 609–618",
    "fontname": "IIWCPO+CMTI8",
    "fontsize": 7.97,
    "page_number": 25,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 91.92,
      "is_bold": 0,
      "y_coordinate": 739.69,
      "centering_offset": 74.13,
      "word_count": 9
    }
  },
  {
    "text": "22. I. Kavasidis, C. Pino, S. Palazzo, F. Rundo, D. Giordano, P. Messina, C. Spampinato,",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 25,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 76.23,
      "is_bold": 0,
      "y_coordinate": 730.53,
      "centering_offset": 50.67,
      "word_count": 15
    }
  },
  {
    "text": "in International Conference on Image Analysis and Processing (Springer, 2019), pp.",
    "fontname": "IIWCPO+CMTI8",
    "fontsize": 7.97,
    "page_number": 25,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 91.92,
      "is_bold": 0,
      "y_coordinate": 721.31,
      "centering_offset": 42.83,
      "word_count": 11
    }
  },
  {
    "text": "292–302\n23. V. Tshitoyan, J. Dagdelen, L. Weston, A. Dunn, Z. Rong, O. Kononova, K.A. Persson,\nG. Ceder, A. Jain, Nature 571(7763), 95 (2019)\n24. D. Nadeau, S. Sekine, Lingvisticae Investigationes 30(1), 3 (2007)\n25. J. Li, A. Sun, J. Han, C. Li, IEEE Transactions on Knowledge and Data Engineering\n(2020)\n26. Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, S. Fidler, in",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 25,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 76.23,
      "is_bold": 0,
      "y_coordinate": 712.1,
      "centering_offset": 50.67,
      "word_count": 67
    }
  },
  {
    "text": "IEEE International Conference on Computer Vision (2015), pp. 19–27",
    "fontname": "IIWCPO+CMTI8",
    "fontsize": 7.97,
    "page_number": 25,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 91.92,
      "is_bold": 0,
      "y_coordinate": 647.81,
      "centering_offset": 73.57,
      "word_count": 9
    }
  },
  {
    "text": "27. C. Sun, Z. Yang, L. Wang, Y. Zhang, H. Lin, J. Wang, Journal of Biomedical Informatics\n103, 103392 (2020)\n28. A. Yates, M. Banko, M. Broadhead, M.J. Cafarella, O. Etzioni, S. Soderland, in An-",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 25,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 76.23,
      "is_bold": 0,
      "y_coordinate": 638.65,
      "centering_offset": 50.67,
      "word_count": 35
    }
  },
  {
    "text": "nual Conference of the North American Chapter of the Association for Computational",
    "fontname": "IIWCPO+CMTI8",
    "fontsize": 7.97,
    "page_number": 25,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 91.92,
      "is_bold": 0,
      "y_coordinate": 611.06,
      "centering_offset": 42.83,
      "word_count": 12
    }
  },
  {
    "text": "Linguistics (2007), pp. 25–26",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 25,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 91.92,
      "is_bold": 0,
      "y_coordinate": 601.85,
      "centering_offset": 151.59,
      "word_count": 4
    }
  },
  {
    "text": "29. F. Wu, D.S. Weld, in 48th Annual Meeting of the Association for Computational Lin-",
    "fontname": "IIWCPO+CMTI8",
    "fontsize": 7.97,
    "page_number": 25,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 76.23,
      "is_bold": 0,
      "y_coordinate": 592.69,
      "centering_offset": 50.67,
      "word_count": 15
    }
  },
  {
    "text": "guistics (2010), pp. 118–127\n30. G. Angeli, M.J.J. Premkumar, C.D. Manning, in 53rd Annual Meeting of the Associa-",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 25,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 76.23,
      "is_bold": 0,
      "y_coordinate": 583.47,
      "centering_offset": 50.67,
      "word_count": 18
    }
  },
  {
    "text": "tion for Computational Linguistics and 7th International Joint Conference on Natural\nLanguage Processing (2015), pp. 344–354\n31. E.F. Tjong Kim Sang, F. De Meulder, in 7th Conference on Natural Language Learning",
    "fontname": "IIWCPO+CMTI8",
    "fontsize": 7.97,
    "page_number": 25,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 76.23,
      "is_bold": 0,
      "y_coordinate": 565.1,
      "centering_offset": 50.67,
      "word_count": 31
    }
  },
  {
    "text": "at HLT-NAACL 2003 (2003), pp. 142–147\n32. Y. Zhang, V. Zhong, D. Chen, G. Angeli, C.D. Manning, in Conference on Empirical",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 25,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 76.23,
      "is_bold": 0,
      "y_coordinate": 537.51,
      "centering_offset": 50.67,
      "word_count": 21
    }
  },
  {
    "text": "Methods in Natural Language Processing (2017), pp. 35–45",
    "fontname": "IIWCPO+CMTI8",
    "fontsize": 7.97,
    "page_number": 25,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 91.92,
      "is_bold": 0,
      "y_coordinate": 519.13,
      "centering_offset": 95.86,
      "word_count": 8
    }
  },
  {
    "text": "33. PDFTron.\nPDF2Text.",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 25,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 76.23,
      "is_bold": 0,
      "y_coordinate": 509.97,
      "centering_offset": 164.23,
      "word_count": 3
    }
  },
  {
    "text": "https://www.pdftron.com/documentation/cli/guides/",
    "fontname": "CMBMGW+CMTT8",
    "fontsize": 7.97,
    "page_number": 25,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 210.23,
      "is_bold": 0,
      "y_coordinate": 509.75,
      "centering_offset": 16.33,
      "word_count": 1
    }
  },
  {
    "text": "pdf2text/ (2021). Online; accessed 15-Feb-2021\n34. C. Ramakrishnan, A. Patnia, E. Hovy, G.A. Burns, Source Code for Biology and Medicine\n7(1), 1 (2012)\n35. M.M. Miro´nczuk, Knowledge and Information Systems 54(3), 711 (2018)\n36. R.B. Tchoua, K. Chard, D. Audus, J. Qin, J. de Pablo, I. Foster, Procedia Computer\nScience 80, 386 (2016)\n37. R.B. Tchoua, K. Chard, D.J. Audus, L.T. Ward, J. Lequieu, J.J. De Pablo, I.T. Foster,",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 25,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 76.23,
      "is_bold": 0,
      "y_coordinate": 500.76,
      "centering_offset": 50.67,
      "word_count": 69
    }
  },
  {
    "text": "in IEEE 13th International Conference on e-Science (IEEE, 2017), pp. 109–118",
    "fontname": "IIWCPO+CMTI8",
    "fontsize": 7.97,
    "page_number": 25,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 91.92,
      "is_bold": 0,
      "y_coordinate": 436.47,
      "centering_offset": 56.98,
      "word_count": 11
    }
  },
  {
    "text": "38. R. Tchoua, A. Ajith, Z. Hong, L. Ward, K. Chard, D. Audus, S. Patel, J. de Pablo,",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 25,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 76.23,
      "is_bold": 0,
      "y_coordinate": 427.32,
      "centering_offset": 50.67,
      "word_count": 18
    }
  },
  {
    "text": "I. Foster, in 15th International Conference on eScience (IEEE, 2019), pp. 126–135",
    "fontname": "IIWCPO+CMTI8",
    "fontsize": 7.97,
    "page_number": 25,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 91.92,
      "is_bold": 0,
      "y_coordinate": 418.1,
      "centering_offset": 52.31,
      "word_count": 12
    }
  },
  {
    "text": "39. Z. Hong, R. Tchoua, K. Chard, I. Foster, in International Conference on Computational\nScience (Springer, 2020), pp. 308–321\n40. R. Tchoua, Z. Hong, D. Audus, S. Patel, L. Ward, K. Chard, J. De Pablo, I. Foster,\nBulletin of the American Physical Society 65 (2020)\n41. L. Von Ahn, B. Maurer, C. McMillen, D. Abraham, M. Blum, Science 321(5895), 1465\n(2008)\n42. F. Hillen, B. H¨oﬂe, International Journal of Applied Earth Observation and Geoinforma-\ntion 40, 29 (2015)\n43. S. Yan, W.S. Spangler, Y. Chen, IEEE/ACM Transactions on Computational Biology\nand Bioinformatics 10(5), 1218 (2013)\n44. A.J. Yepes, A. MacKinlay, N. Gunn, C. Schieber, N. Faux, M. Downton, B. Goudey,\nR.L. Martin, in AMIA Annual Symposium Proceedings, vol. 2018 (American Medical\nInformatics Association, 2018), vol. 2018, p. 616\n45. K. Ganchev, F. Pereira, M. Mandel, S. Carroll, P. White, in Proceedings of the linguistic",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 25,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 76.23,
      "is_bold": 0,
      "y_coordinate": 408.94,
      "centering_offset": 50.67,
      "word_count": 144
    }
  },
  {
    "text": "annotation workshop (2007), pp. 53–56\n46. Y. Jo, E. Mayﬁeld, C. Reed, E. Hovy, in 12th Language Resources and Evaluation Con-",
    "fontname": "IIWCPO+CMTI8",
    "fontsize": 7.97,
    "page_number": 25,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 76.23,
      "is_bold": 0,
      "y_coordinate": 280.26,
      "centering_offset": 50.67,
      "word_count": 21
    }
  },
  {
    "text": "ference (2020), pp. 1008–1018\n47. Z. Hong, J.G. Pauloski, L. Ward, K. Chard, B. Blaiszik, I. Foster, arXiv preprint\narXiv:2101.04617 (2021)\n48. K. Lybarger, M. Ostendorf, M. Yetisgen, Journal of Biomedical Informatics 113, 103631\n(2021)\n49. S.M. Swanberg, Journal of the Medical Library Association 105(1), 106 (2017)",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 25,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 76.23,
      "is_bold": 0,
      "y_coordinate": 261.89,
      "centering_offset": 50.67,
      "word_count": 47
    }
  },
  {
    "text": "50. I. Beltagy, K. Lo, A. Cohan, in Conference on Empirical Methods in Natural Language\nProcessing (2019)",
    "fontname": "IIWCPO+CMTI8",
    "fontsize": 7.97,
    "page_number": 25,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 76.23,
      "is_bold": 0,
      "y_coordinate": 206.82,
      "centering_offset": 50.67,
      "word_count": 17
    }
  },
  {
    "text": "26\nZhi Hong et al.\n51. M. Marcus, B. Santorini, M.A. Marcinkiewicz, Building a large annotated corpus of\nEnglish: The Penn Treebank. Technical Report MS-CIS-93-8, University of Pennsylvania,\nDepartment of Computer and Information Science (1993)\n52. K. Bontcheva, I. Roberts, L. Derczynski, S. Alexander-Eames, in Demonstrations at the",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 26,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 774.32,
      "centering_offset": 52.78,
      "word_count": 48
    }
  },
  {
    "text": "14th Conference of the European Chapter of the Association for Computational Linguis-",
    "fontname": "IIWCPO+CMTI8",
    "fontsize": 7.97,
    "page_number": 26,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 91.92,
      "is_bold": 0,
      "y_coordinate": 711.75,
      "centering_offset": 42.83,
      "word_count": 12
    }
  },
  {
    "text": "tics (2014), pp. 97–100\n53. B.M. Good, M. Nanis, C. Wu, A.I. Su, in Paciﬁc Symposium on Biocomputing (World\nScientiﬁc, 2014), pp. 282–293\n54. C.G. Northcutt, A. Athalye, J. Mueller, arXiv preprint arXiv:2103.14749 (2021)\n55. R.B. Tchoua, J. Qin, D.J. Audus, K. Chard, I.T. Foster, J. de Pablo, Journal of Chemical\nEducation 93(9), 1561 (2016)\n56. M. Krallinger, O. Rabal, F. Leitner, M. Vazquez, D. Salgado, Z. Lu, R. Leaman, Y. Lu,\nD. Ji, D.M. Lowe, R.A. Sayle, R.T. Batista-Navarro, R. Rak, T. Huber, T. Rockt¨aschel,\nS. Matos, D. Campos, B. Tang, H. Xu, T. Munkhdalai, K.H. Ryu, S. Ramanan, S. Nathan,\nS. ˇZitnik, M. Bajec, L. Weber, M. Irmer, S.A. Akhondi, J.A. Kors, S. Xu, X. An, U.K.\nSikdar, A. Ekbal, M. Yoshioka, T.M. Dieb, M. Choi, K. Verspoor, M. Khabsa, C.L.\nGiles, H. Liu, K.E. Ravikumar, A. Lamurias, F.M. Couto, H.J. Dai, R.T.H. Tsai, C. Ata,\nT. Can, A. Usi´e, R. Alves, I. Segura-Bedmar, P. Mart´ınez, J. Oyarzabal, A. Valencia,\nJournal of Cheminformatics 7(1), 1 (2015)\n57. S. Mysore, Z. Jensen, E. Kim, K. Huang, H.S. Chang, E. Strubell, J. Flanigan, A. McCal-\nlum, E. Olivetti, in 13th Linguistic Annotation Workshop (Association for Computational\nLinguistics, 2019), pp. 56–64\n58. A. Peskin, A. Dima, Integrating Materials and Manufacturing Innovation 6(2), 187 (2017)\n59. L. Von Ahn, Computer 39(6), 92 (2006)\n60. A. Kawrykow, G. Roumanis, A. Kam, D. Kwak, C. Leung, C. Wu, E. Zarour, L. Sarmenta,\nM. Blanchette, J. Waldisp¨uhl, PloS one 7(3), e31362 (2012)",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 26,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 76.23,
      "is_bold": 0,
      "y_coordinate": 702.53,
      "centering_offset": 50.67,
      "word_count": 247
    }
  },
  {
    "text": "61. B. Guillaume, K. Fort, N. Lefebvre, in International Conference on Computational Lin-\nguistics (2016)\n62. H.A. Favre, W.H. Powell, Nomenclature of organic chemistry: IUPAC recommendations",
    "fontname": "IIWCPO+CMTI8",
    "fontsize": 7.97,
    "page_number": 26,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 76.23,
      "is_bold": 0,
      "y_coordinate": 506.38,
      "centering_offset": 50.67,
      "word_count": 26
    }
  },
  {
    "text": "and preferred names 2013 (Royal Society of Chemistry, London, UK, 2013)\n63. H.L. Morgan, Journal of Chemical Documentation 5(2), 107 (1965)\n64. C. Bizer, J. Lehmann, G. Kobilarov, S. Auer, C. Becker, R. Cyganiak, S. Hellmann,\nJournal of Web Semantics 7(3), 154 (2009)\n65. B. Settles, Synthesis Lectures on Artiﬁcial Intelligence and Machine Learning 6(1), 1\n(2012)",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 26,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 76.23,
      "is_bold": 0,
      "y_coordinate": 478.44,
      "centering_offset": 50.67,
      "word_count": 57
    }
  },
  {
    "text": "66. A.R. Camacho, in 14th IAPR International Workshop on Document Analysis Systems,",
    "fontname": "IIWCPO+CMTI8",
    "fontsize": 7.97,
    "page_number": 26,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 76.23,
      "is_bold": 0,
      "y_coordinate": 421.98,
      "centering_offset": 50.67,
      "word_count": 12
    }
  },
  {
    "text": "vol. 12116 (Springer Nature, 2020), vol. 12116, p. 324\n67. M. Mintz, S. Bills, R. Snow, D. Jurafsky, in Joint Conference of the 47th Annual Meeting",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 26,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 76.23,
      "is_bold": 0,
      "y_coordinate": 412.77,
      "centering_offset": 50.67,
      "word_count": 26
    }
  },
  {
    "text": "of the ACL and the 4th International Joint Conference on Natural Language Processing",
    "fontname": "IIWCPO+CMTI8",
    "fontsize": 7.97,
    "page_number": 26,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 91.92,
      "is_bold": 0,
      "y_coordinate": 394.05,
      "centering_offset": 42.83,
      "word_count": 13
    }
  },
  {
    "text": "of the AFNLP (2009), pp. 1003–1011",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 26,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 91.92,
      "is_bold": 0,
      "y_coordinate": 384.83,
      "centering_offset": 136.36,
      "word_count": 6
    }
  },
  {
    "text": "68. S. Riedel, L. Yao, A. McCallum, in Joint European Conference on Machine Learning\nand Knowledge Discovery in Databases (Springer, 2010), pp. 148–163",
    "fontname": "IIWCPO+CMTI8",
    "fontsize": 7.97,
    "page_number": 26,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 76.23,
      "is_bold": 0,
      "y_coordinate": 375.32,
      "centering_offset": 50.67,
      "word_count": 23
    }
  },
  {
    "text": "69. M. Surdeanu, J. Tibshirani, R. Nallapati, C.D. Manning, in Joint Conference on Em-",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 26,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 76.23,
      "is_bold": 0,
      "y_coordinate": 356.6,
      "centering_offset": 50.67,
      "word_count": 14
    }
  },
  {
    "text": "pirical Methods in Natural Language Processing and Computational Natural Language",
    "fontname": "IIWCPO+CMTI8",
    "fontsize": 7.97,
    "page_number": 26,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 91.92,
      "is_bold": 0,
      "y_coordinate": 347.38,
      "centering_offset": 42.83,
      "word_count": 10
    }
  },
  {
    "text": "Learning (2012), pp. 455–465",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 26,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 91.92,
      "is_bold": 0,
      "y_coordinate": 338.17,
      "centering_offset": 151.21,
      "word_count": 4
    }
  },
  {
    "text": "70. T. Liu, K. Wang, B. Chang, Z. Sui, in Conference on Empirical Methods in Natural",
    "fontname": "IIWCPO+CMTI8",
    "fontsize": 7.97,
    "page_number": 26,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 76.23,
      "is_bold": 0,
      "y_coordinate": 328.66,
      "centering_offset": 50.67,
      "word_count": 16
    }
  },
  {
    "text": "Language Processing (2017), pp. 1790–1795\n71. W. Xu, R. Hoﬀmann, L. Zhao, R. Grishman, in 51st Annual Meeting of the Association",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 26,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 76.23,
      "is_bold": 0,
      "y_coordinate": 319.45,
      "centering_offset": 50.67,
      "word_count": 21
    }
  },
  {
    "text": "for Computational Linguistics (Volume 2: Short Papers) (2013), pp. 665–670",
    "fontname": "IIWCPO+CMTI8",
    "fontsize": 7.97,
    "page_number": 26,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 91.92,
      "is_bold": 0,
      "y_coordinate": 300.72,
      "centering_offset": 62.16,
      "word_count": 10
    }
  },
  {
    "text": "72. T. Onishi, T. Kadohira, I. Watanabe, Science and Technology of Advanced Materials\n19(1), 649 (2018)\n73. K. Ravikumar, H. Liu, J.D. Cohn, M.E. Wall, K. Verspoor, Journal of Biomedical Se-\nmantics 3(3), 1 (2012)",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 26,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 76.23,
      "is_bold": 0,
      "y_coordinate": 291.22,
      "centering_offset": 50.67,
      "word_count": 35
    }
  },
  {
    "text": "74. C. Quirk, H. Poon, in 15th Conference of the European Chapter of the Association for\nComputational Linguistics (2017), pp. 1171–1182",
    "fontname": "IIWCPO+CMTI8",
    "fontsize": 7.97,
    "page_number": 26,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 76.23,
      "is_bold": 0,
      "y_coordinate": 253.77,
      "centering_offset": 50.67,
      "word_count": 21
    }
  },
  {
    "text": "75. D. Buscaldi, D. Dess`ı, E. Motta, F. Osborne, D.R. Recupero, in European Semantic Web\nConference (Springer, 2019), pp. 8–12",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 26,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 76.23,
      "is_bold": 0,
      "y_coordinate": 235.05,
      "centering_offset": 50.67,
      "word_count": 20
    }
  },
  {
    "text": "76. A. Fader, S. Soderland, O. Etzioni, in Conference on Empirical Methods in Natural",
    "fontname": "IIWCPO+CMTI8",
    "fontsize": 7.97,
    "page_number": 26,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 76.23,
      "is_bold": 0,
      "y_coordinate": 216.33,
      "centering_offset": 50.67,
      "word_count": 14
    }
  },
  {
    "text": "Language Processing (2011), pp. 1535–1545\n77. S. Soderland, B. Roof, B. Qin, S. Xu, O. Etzioni, AI Magazine 31(3), 93 (2010)",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 26,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 76.23,
      "is_bold": 0,
      "y_coordinate": 207.11,
      "centering_offset": 65.56,
      "word_count": 21
    }
  },
  {
    "text": "Title Suppressed Due to Excessive Length\n27\n78. Y. Luan, L. He, M. Ostendorf, H. Hajishirzi, in Conference on Empirical Methods in",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 27,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 774.32,
      "centering_offset": 52.79,
      "word_count": 22
    }
  },
  {
    "text": "Natural Language Processing (2018), pp. 3219–3232",
    "fontname": "IIWCPO+CMTI8",
    "fontsize": 7.97,
    "page_number": 27,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 91.92,
      "is_bold": 0,
      "y_coordinate": 739.69,
      "centering_offset": 109.47,
      "word_count": 6
    }
  },
  {
    "text": "79. R. Kruiper, J.F. Vincent, J. Chen-Burger, M.P. Desmulliez, I. Konstas, arXiv preprint\narXiv:2005.07751 (2020)\n80. K. White, Publications output: US trends and international comparisons. Tech. rep.,",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 27,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 76.23,
      "is_bold": 0,
      "y_coordinate": 730.57,
      "centering_offset": 50.67,
      "word_count": 27
    }
  },
  {
    "text": "National Science Foundation (2019). https://ncses.nsf.gov/pubs/nsb20206/",
    "fontname": "CMBMGW+CMTT8",
    "fontsize": 7.97,
    "page_number": 27,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 91.92,
      "is_bold": 0,
      "y_coordinate": 703.03,
      "centering_offset": 60.18,
      "word_count": 5
    }
  },
  {
    "text": "81. E. Riloﬀ, in 11th National Conference on Artiﬁcial intelligence (1993), pp. 811–816",
    "fontname": "IIWCPO+CMTI8",
    "fontsize": 7.97,
    "page_number": 27,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 76.23,
      "is_bold": 0,
      "y_coordinate": 693.91,
      "centering_offset": 58.11,
      "word_count": 13
    }
  },
  {
    "text": "82. S. Soderland, Machine Learning 34(1), 233 (1999)\n83. E. Murphy, Ensemble labeling towards scientiﬁc information extraction (ELSIE). Ph.D.\nthesis, College of Computing and Digital Media (2020)\n84. I. Hendrickx, S.N. Kim, Z. Kozareva, P. Nakov, D. ´O S´eaghdha, S. Pad´o, M. Pennac-",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 27,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 76.23,
      "is_bold": 0,
      "y_coordinate": 684.8,
      "centering_offset": 50.67,
      "word_count": 43
    }
  },
  {
    "text": "chiotti, L. Romano, S. Szpakowicz, in 5th International Workshop on Semantic Evalua-",
    "fontname": "IIWCPO+CMTI8",
    "fontsize": 7.97,
    "page_number": 27,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 91.92,
      "is_bold": 0,
      "y_coordinate": 646.82,
      "centering_offset": 42.83,
      "word_count": 12
    }
  },
  {
    "text": "tion (Association for Computational Linguistics, 2010), pp. 33–38\n85. D.D.A. Bui, G. Del Fiol, S. Jonnalagadda, Journal of Biomedical Informatics 61, 141\n(2016)\n86. C. Blaschke, L. Hirschman, A. Valencia, Brieﬁngs in Bioinformatics 3(2), 154 (2002)\n87. K.B. Cohen, K. Verspoor, H.L. Johnson, C. Roeder, P. Ogren, W.A. Baumgartner Jr,",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 27,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 76.23,
      "is_bold": 0,
      "y_coordinate": 637.6,
      "centering_offset": 50.67,
      "word_count": 50
    }
  },
  {
    "text": "E. White, L. Hunter, in BioNLP 2009 Workshop Companion Volume for Shared Task",
    "fontname": "IIWCPO+CMTI8",
    "fontsize": 7.97,
    "page_number": 27,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 91.92,
      "is_bold": 0,
      "y_coordinate": 591.83,
      "centering_offset": 42.83,
      "word_count": 13
    }
  },
  {
    "text": "(2009), pp. 50–58\n88. Q.L. Nguyen, D. Tikk, U. Leser, Journal of Biomedical Semantics 1(1), 1 (2010)\n89. V. Pillet, M´ethodologie d’extraction automatique d’information `a partir de la litt´erature\nscientiﬁque en vue d’alimenter un nouveau syst`eme d’information: application `a la\ng´en´etique mol´eculaire pour l’extraction d’information sur les interactions. Ph.D. the-\nsis, Univ. d’Aix-Marseille 3 (2000)\n90. J.R. Quinlan, C4.5: Programs for Machine Learning (Morgan Kaufmann Publishers Inc.,\nSan Francisco, CA, USA, 1993)",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 27,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 76.23,
      "is_bold": 0,
      "y_coordinate": 582.62,
      "centering_offset": 50.67,
      "word_count": 72
    }
  },
  {
    "text": "91. E. Riloﬀ, J. Wiebe, T. Wilson, in 7th Conference on Natural Language Learning (2003),",
    "fontname": "IIWCPO+CMTI8",
    "fontsize": 7.97,
    "page_number": 27,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 76.23,
      "is_bold": 0,
      "y_coordinate": 509.3,
      "centering_offset": 50.67,
      "word_count": 15
    }
  },
  {
    "text": "pp. 25–32\n92. E. Riloﬀ, J. Wiebe, W. Phillips, in AAAI (2005), pp. 1106–1111",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 27,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 76.23,
      "is_bold": 0,
      "y_coordinate": 500.08,
      "centering_offset": 94.03,
      "word_count": 14
    }
  },
  {
    "text": "93. J. Wiebe, E. Riloﬀ, in International Conference on Intelligent Text Processing and Com-",
    "fontname": "IIWCPO+CMTI8",
    "fontsize": 7.97,
    "page_number": 27,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 76.23,
      "is_bold": 0,
      "y_coordinate": 481.86,
      "centering_offset": 50.67,
      "word_count": 14
    }
  },
  {
    "text": "putational Linguistics (Springer, 2005), pp. 486–497\n94. J. Wiebe, E. Riloﬀ, IEEE Transactions on Aﬀective Computing 2(4), 175 (2011)\n95. C. N´edellec, M.O.A. Vetah, P. Bessieres, in European Conference on Principles of Data",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 27,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 76.23,
      "is_bold": 0,
      "y_coordinate": 472.64,
      "centering_offset": 50.67,
      "word_count": 33
    }
  },
  {
    "text": "Mining and Knowledge Discovery (Springer, 2001), pp. 326–337",
    "fontname": "IIWCPO+CMTI8",
    "fontsize": 7.97,
    "page_number": 27,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 91.92,
      "is_bold": 0,
      "y_coordinate": 445.2,
      "centering_offset": 86.84,
      "word_count": 8
    }
  },
  {
    "text": "96. A.H. Aliwy, E.A. Ameer, International Journal of Applied Engineering Research 12(14),\n4309 (2017)\n97. A. Ratner, S.H. Bach, H. Ehrenberg, J. Fries, S. Wu, C. R´e, International Conference on\nVery Large Data Bases 11(3), 269 (2017)\n98. A.J. Ratner, S.H. Bach, H.R. Ehrenberg, C. R´e, in ACM International Conference on\nManagement of Data (2017), pp. 1683–1686\n99. E.F. Sang, F. De Meulder, arXiv preprint cs/0306050 (2003)\n100. R. Weischedel, S. Pradhan, L. Ramshaw, M. Palmer, N. Xue, M. Marcus, A. Taylor,\nC. Greenberg, E. Hovy, R. Belvin, A. Houston, OntoNotes Release 5.0. Web download,\nLinguistic Data Consortium (2013). DOI 10.35111/xmhb-2b84. https://catalog.ldc.",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 27,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 436.09,
      "centering_offset": 52.79,
      "word_count": 102
    }
  },
  {
    "text": "upenn.edu/LDC2013T19",
    "fontname": "CMBMGW+CMTT8",
    "fontsize": 7.97,
    "page_number": 27,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 91.92,
      "is_bold": 0,
      "y_coordinate": 344.11,
      "centering_offset": 163.38,
      "word_count": 1
    }
  },
  {
    "text": "101. J. Pennington, R. Socher, C.D. Manning, in Conference on Empirical Methods in Natural\nLanguage Processing (2014), pp. 1532–1543\n102. T. Mikolov, E. Grave, P. Bojanowski, C. Puhrsch, A. Joulin, in International Conference",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 27,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 335.22,
      "centering_offset": 52.79,
      "word_count": 33
    }
  },
  {
    "text": "on Language Resources and Evaluation (2018)",
    "fontname": "IIWCPO+CMTI8",
    "fontsize": 7.97,
    "page_number": 27,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 91.92,
      "is_bold": 0,
      "y_coordinate": 307.68,
      "centering_offset": 119.19,
      "word_count": 6
    }
  },
  {
    "text": "103. J. Devlin, M.W. Chang, K. Lee, K. Toutanova, in Conference of the North American",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 27,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 298.57,
      "centering_offset": 52.79,
      "word_count": 15
    }
  },
  {
    "text": "Chapter of the Association for Computational Linguistics: Human Language Technolo-",
    "fontname": "IIWCPO+CMTI8",
    "fontsize": 7.97,
    "page_number": 27,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 91.92,
      "is_bold": 0,
      "y_coordinate": 289.35,
      "centering_offset": 42.83,
      "word_count": 10
    }
  },
  {
    "text": "gies (Association for Computational Linguistics, 2019), pp. 4171–4186\n104. C. Rosset, Microsoft Research Blog (2020). https://bit.ly/3eF1coS\n105. H. Saif, M. Fernandez, Y. He, H. Alani, in 1st International Workshop on Emotion and",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 27,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 280.14,
      "centering_offset": 52.79,
      "word_count": 32
    }
  },
  {
    "text": "Sentiment in Social and Expressive Media: Approaches and Perspectives from AI (2013)",
    "fontname": "IIWCPO+CMTI8",
    "fontsize": 7.97,
    "page_number": 27,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 91.92,
      "is_bold": 0,
      "y_coordinate": 252.69,
      "centering_offset": 42.83,
      "word_count": 12
    }
  },
  {
    "text": "106. A.L. Maas, R.E. Daly, P.T. Pham, D. Huang, A.Y. Ng, C. Potts, in 49th Annual Meet-",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 27,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 243.58,
      "centering_offset": 52.79,
      "word_count": 17
    }
  },
  {
    "text": "ing of the Association for Computational Linguistics: Human Language Technologies",
    "fontname": "IIWCPO+CMTI8",
    "fontsize": 7.97,
    "page_number": 27,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 91.92,
      "is_bold": 0,
      "y_coordinate": 234.36,
      "centering_offset": 42.83,
      "word_count": 10
    }
  },
  {
    "text": "(Association for Computational Linguistics, 2011), pp. 142–150\n107. H. Elsahar, P. Vougiouklis, A. Remaci, C. Gravier, J. Hare, F. Laforest, E. Simperl, in 11th",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 27,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 225.15,
      "centering_offset": 52.79,
      "word_count": 24
    }
  },
  {
    "text": "International Conference on Language Resources and Evaluation (European Language",
    "fontname": "IIWCPO+CMTI8",
    "fontsize": 7.97,
    "page_number": 27,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 91.92,
      "is_bold": 0,
      "y_coordinate": 206.82,
      "centering_offset": 42.83,
      "word_count": 9
    }
  },
  {
    "text": "Resources Association, 2018)",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 27,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 91.92,
      "is_bold": 0,
      "y_coordinate": 197.6,
      "centering_offset": 152.17,
      "word_count": 3
    }
  },
  {
    "text": "28\nZhi Hong et al.",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 28,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 774.32,
      "centering_offset": 52.78,
      "word_count": 5
    }
  },
  {
    "text": "108. W. Sun, X. Peng, X. Wan, in Proceedings of the Sixth International Joint Conference on\nNatural Language Processing (2013), pp. 180–188",
    "fontname": "IIWCPO+CMTI8",
    "fontsize": 7.97,
    "page_number": 28,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 748.9,
      "centering_offset": 52.79,
      "word_count": 22
    }
  },
  {
    "text": "109. A. Trask, P. Michalak, J. Liu, arXiv preprint arXiv:1511.06388 (2015)\n110. P. Groth, M. Lauruhn, A. Scerri, R. Daniel Jr, arXiv preprint arXiv:1802.05574 (2018)\n111. E. Kim, K. Huang, A. Tomala, S. Matthews, E. Strubell, A. Saunders, A. McCallum,\nE. Olivetti, Scientiﬁc Data 4(1), 1 (2017)\n112. E. Kim, Z. Jensen, A. van Grootel, K. Huang, M. Staib, S. Mysore, H.S. Chang,\nE. Strubell, A. McCallum, S. Jegelka, E. Olivetti, Journal of Chemical Information and\nModeling 60(3), 1194 (2020)\n113. D.S. Maitra, U. Bhattacharya, S.K. Parui, in 13th International Conference on Document\nAnalysis and Recognition (IEEE, 2015), pp. 1021–1025\n114. Y. Wu, M. Schuster, Z. Chen, Q.V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao,\nQ. Gao, K. Macherey, J. Klingner, A. Shah, M. Johnson, X. Liu, (cid:32)Lukasz Kaiser, S. Gouws,\nY. Kato, T. Kudo, H. Kazawa, K. Stevens, G. Kurian, N. Patil, W. Wang, C. Young,\nJ. Smith, J. Riesa, A. Rudnick, O. Vinyals, G. Corrado, M. Hughes, J. Dean, arXiv\npreprint arXiv:1609.08144 (2016)\n115. C.B. Do, A.Y. Ng, Advances in Neural Information Processing Systems 18, 299 (2005)\n116. M. Raghu, C. Zhang, J. Kleinberg, S. Bengio, in 33rd Conference on Neural Information",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 28,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 730.23,
      "centering_offset": 52.79,
      "word_count": 196
    }
  },
  {
    "text": "Processing Systems (2019)",
    "fontname": "IIWCPO+CMTI8",
    "fontsize": 7.97,
    "page_number": 28,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 91.92,
      "is_bold": 0,
      "y_coordinate": 581.1,
      "centering_offset": 156.48,
      "word_count": 3
    }
  },
  {
    "text": "117. H. Yamada, C. Liu, S. Wu, Y. Koyama, S. Ju, J. Shiomi, J. Morikawa, R. Yoshida, ACS\nCentral Science 5(10), 1717 (2019)\n118. Y. Gong, H. Shao, J. Luo, Z. Li, Composite Structures 252, 112681 (2020)\n119. T. Mikolov, K. Chen, G. Corrado, J. Dean, arXiv preprint arXiv:1301.3781 (2013)\n120. T. Mikolov, I. Sutskever, K. Chen, G. Corrado, J. Dean, in 26th International Conference",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 28,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 571.64,
      "centering_offset": 52.79,
      "word_count": 65
    }
  },
  {
    "text": "on Neural Information Processing Systems (2013), pp. 3111–3119",
    "fontname": "IIWCPO+CMTI8",
    "fontsize": 7.97,
    "page_number": 28,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 91.92,
      "is_bold": 0,
      "y_coordinate": 524.84,
      "centering_offset": 83.7,
      "word_count": 8
    }
  },
  {
    "text": "121. T.B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan,\nP. Shyam, G. Sastry, A. Askell, et al., arXiv preprint arXiv:2005.14165 (2020)\n122. Google. Google News Word2Vec.",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 28,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 515.38,
      "centering_offset": 52.79,
      "word_count": 32
    }
  },
  {
    "text": "https://code.google.com/archive/p/word2vec/",
    "fontname": "CMBMGW+CMTT8",
    "fontsize": 7.97,
    "page_number": 28,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 235.63,
      "is_bold": 0,
      "y_coordinate": 496.49,
      "centering_offset": 29.03,
      "word_count": 1
    }
  },
  {
    "text": "(2021). Online; accessed 07-Apr-2021\n123. ´E. Grave, P. Bojanowski, P. Gupta, A. Joulin, T. Mikolov, in 11th International Confer-",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 28,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 487.5,
      "centering_offset": 52.78,
      "word_count": 19
    }
  },
  {
    "text": "ence on Language Resources and Evaluation (2018)",
    "fontname": "IIWCPO+CMTI8",
    "fontsize": 7.97,
    "page_number": 28,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 91.92,
      "is_bold": 0,
      "y_coordinate": 467.5,
      "centering_offset": 109.57,
      "word_count": 7
    }
  },
  {
    "text": "124. I. Sutskever, O. Vinyals, Q.V. Le, arXiv preprint arXiv:1409.3215 (2014)\n125. A.H. Larsen, J.J. Mortensen, J. Blomqvist, I.E. Castelli, R. Christensen, M. Du(cid:32)lak,\nJ. Friis, M.N. Groves, B. Hammer, C. Hargus, E.D. Hermes, P.C. Jennings, P.B. Jensen,\nJ. Kermode, J.R. Kitchin, E.L. Kolsbjerg, J. Kubal, K. Kaasbjerg, S. Lysgaard, J.B.\nMaronsson, T. Maxson, T. Olsen, L. Pastewka, A. Peterson, C. Rostgaard, J. Schiøtz,\nO. Sch¨utt, M. Strange, K.S. Thygesen, T. Vegge, L. Vilhelmsen, M. Walter, Z. Zeng,\nK.W. Jacobsen, Journal of Physics: Condensed Matter 29(27), 273002 (2017). DOI",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 28,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 458.05,
      "centering_offset": 52.78,
      "word_count": 89
    }
  },
  {
    "text": "10.1088/1361-648x/aa680e. URL https://doi.org/10.1088/1361-648x/aa680e",
    "fontname": "CMBMGW+CMTT8",
    "fontsize": 7.97,
    "page_number": 28,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 91.92,
      "is_bold": 0,
      "y_coordinate": 393.3,
      "centering_offset": 58.1,
      "word_count": 3
    }
  },
  {
    "text": "126. M.C. Swain, J.M. Cole, Journal of chemical information and modeling 56(10), 1894 (2016)\n127. S.R. Hall, F.H. Allen, I.D. Brown, Acta Crystallographica Section A: Foundations of\nCrystallography 47(6), 655 (1991)\n128. C. Draxl, M. Scheﬄer, MRS Bulletin 43(9), 676 (2018)\n129. B. Blaiszik, K. Chard, J. Pruyne, R. Ananthakrishnan, S. Tuecke, I. Foster, Journal of\nMaterials (2016)\n130. B. Blaiszik, L. Ward, M. Schwarting, J. Gaﬀ, R. Chard, D. Pike, K. Chard, I. Foster,\nMRS Communications 9(4), 1125 (2019)\n131. M.R. Seringhaus, M.B. Gerstein, BMC bioinformatics 8(1), 1 (2007)\n132. B. Mons, H. van Haagen, C. Chichester, J.T. den Dunnen, G. van Ommen, E. van Mul-\nligen, B. Singh, R. Hooft, M. Roos, J. Hammond, et al., Nature genetics 43(4), 281\n(2011)\n133. M. Frenkel, R.D. Chiroco, V. Diky, Q. Dong, K.N. Marsh, J.H. Dymond, W.A. Wake-\nham, S.E. Stein, E. K¨onigsberger, A.R.H. Goodwin, Pure and Applied Chemistry\n78(3), 541 (2006). DOI 10.1351/pac200678030541. URL https://doi.org/10.1351/",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 28,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 383.84,
      "centering_offset": 52.79,
      "word_count": 156
    }
  },
  {
    "text": "pac200678030541",
    "fontname": "CMBMGW+CMTT8",
    "fontsize": 7.97,
    "page_number": 28,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 91.92,
      "is_bold": 0,
      "y_coordinate": 243.7,
      "centering_offset": 173.96,
      "word_count": 1
    }
  },
  {
    "text": "134. C.W. Andersen, R. Armiento, E. Blokhin, G.J. Conduit, S. Dwaraknath, M.L. Evans,\n´A. Fekete, A. Gopakumar, S. Graˇzulis, A. Merkys, F. Mohamed, C. Oses, G. Pizzi,\nG.M. Rignanese, M. Scheidgen, L. Talirz, C. Toher, D. Winston, R. Aversa, K. Choud-\nhary, P. Colinet, S. Curtarolo, D.D. Stefano, C. Draxl, S. Er, M. Esters, M. Fornari,\nM. Giantomassi, M. Govoni, G. Hautier, V. Hegde, M.K. Horton, P. Huck, G. Huhs,",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 28,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 234.47,
      "centering_offset": 52.79,
      "word_count": 70
    }
  },
  {
    "text": "Title Suppressed Due to Excessive Length\n29\nJ. Hummelshøj, A. Kariryaa, B. Kozinsky, S. Kumbhar, M. Liu, N. Marzari, A.J. Mor-\nris, A.A. Mostoﬁ, K.A. Persson, G. Petretto, T. Purcell, F. Ricci, F. Rose, M. Schef-\nﬂer, D. Speckhard, M. Uhrin, A. Vaitkus, P. Villars, D. Waroquiers, C. Wolverton,\nM. Wu, X. Yang, Scientiﬁc Data 8(1) (2021). DOI 10.1038/s41597-021-00974-z. URL",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 29,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 774.32,
      "centering_offset": 52.79,
      "word_count": 60
    }
  },
  {
    "text": "https://doi.org/10.1038/s41597-021-00974-z",
    "fontname": "CMBMGW+CMTT8",
    "fontsize": 7.97,
    "page_number": 29,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 91.92,
      "is_bold": 0,
      "y_coordinate": 711.82,
      "centering_offset": 116.05,
      "word_count": 1
    }
  },
  {
    "text": "135. L. Ward, M. Aykol, B. Blaiszik, I. Foster, B. Meredig, J. Saal, S. Suram, MRS Bulletin\n43(9), 683 (2018). DOI 10.1557/mrs.2018.204. URL https://doi.org/10.1557/mrs.",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 29,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 702.82,
      "centering_offset": 52.79,
      "word_count": 24
    }
  },
  {
    "text": "2018.204",
    "fontname": "CMBMGW+CMTT8",
    "fontsize": 7.97,
    "page_number": 29,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 91.92,
      "is_bold": 0,
      "y_coordinate": 684.17,
      "centering_offset": 188.78,
      "word_count": 1
    }
  },
  {
    "text": "136. D. Metzler, Y. Tay, D. Bahri, M. Najork, arXiv preprint arXiv:2105.02274 (2021)",
    "fontname": "OOPQLW+CMR8",
    "fontsize": 7.97,
    "page_number": 29,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 7.97,
      "indentation": 72.0,
      "is_bold": 0,
      "y_coordinate": 675.18,
      "centering_offset": 64.72,
      "word_count": 13
    }
  },
  {
    "text": "View publication stats",
    "fontname": "Helvetica",
    "fontsize": 4.0,
    "page_number": 29,
    "document_name": "NLP_Review_JOM",
    "features": {
      "font_size": 4.0,
      "indentation": 50.0,
      "is_bold": 0,
      "y_coordinate": 8.17,
      "centering_offset": 228.41,
      "word_count": 3
    }
  }
]